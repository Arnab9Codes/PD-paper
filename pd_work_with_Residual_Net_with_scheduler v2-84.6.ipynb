{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "language_info": {
      "name": "python",
      "version": "3.7.9",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "name": "  pd-work with Residual Net with scheduler.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Arnab9Codes/PD-paper/blob/main/pd_work_with_Residual_Net_with_scheduler%20v2-84.6.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vg72QPAqH-xY",
        "outputId": "f55da503-b7a1-47eb-abb0-be9b37f3df15"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eu8DgIgururO",
        "trusted": true
      },
      "source": [
        "## Initial Imports\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "import pandas as pd \n",
        "import numpy as np \n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import random\n",
        "from tqdm import tqdm_notebook as tqdm\n",
        "\n",
        "#import lime\n",
        "#import eli5\n",
        "#import mlxtend\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CwDybsyfM-2V"
      },
      "source": [
        "np.random.seed(10)"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o4fLLdJVrurY",
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "34dd983f-0ff3-4747-9110-2b467c38d47d"
      },
      "source": [
        "\n",
        "pd_speech_features = pd.read_csv('/content/drive/My Drive/Paper work/Speech detection/pd_speech_features.csv')\n",
        "new_header = pd_speech_features.iloc[0] \n",
        "pd_speech_features = pd_speech_features[1:]\n",
        "pd_speech_features.columns = new_header \n",
        "pd_speech_features.head()\n",
        "print('Shape of the matrix is :', pd_speech_features.shape)"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Shape of the matrix is : (756, 755)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7NIsR5gTrurh",
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 307
        },
        "outputId": "1fe7da81-3725-4a65-e373-a41865a8da65"
      },
      "source": [
        "pd_speech_features.info() # Gives type of columns\n",
        "pd_speech_features.describe()"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 756 entries, 1 to 756\n",
            "Columns: 755 entries, id to class\n",
            "dtypes: object(755)\n",
            "memory usage: 4.4+ MB\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>gender</th>\n",
              "      <th>PPE</th>\n",
              "      <th>DFA</th>\n",
              "      <th>RPDE</th>\n",
              "      <th>numPulses</th>\n",
              "      <th>numPeriodsPulses</th>\n",
              "      <th>meanPeriodPulses</th>\n",
              "      <th>stdDevPeriodPulses</th>\n",
              "      <th>locPctJitter</th>\n",
              "      <th>locAbsJitter</th>\n",
              "      <th>rapJitter</th>\n",
              "      <th>ppq5Jitter</th>\n",
              "      <th>ddpJitter</th>\n",
              "      <th>locShimmer</th>\n",
              "      <th>locDbShimmer</th>\n",
              "      <th>apq3Shimmer</th>\n",
              "      <th>apq5Shimmer</th>\n",
              "      <th>apq11Shimmer</th>\n",
              "      <th>ddaShimmer</th>\n",
              "      <th>meanAutoCorrHarmonicity</th>\n",
              "      <th>meanNoiseToHarmHarmonicity</th>\n",
              "      <th>meanHarmToNoiseHarmonicity</th>\n",
              "      <th>minIntensity</th>\n",
              "      <th>maxIntensity</th>\n",
              "      <th>meanIntensity</th>\n",
              "      <th>f1</th>\n",
              "      <th>f2</th>\n",
              "      <th>f3</th>\n",
              "      <th>f4</th>\n",
              "      <th>b1</th>\n",
              "      <th>b2</th>\n",
              "      <th>b3</th>\n",
              "      <th>b4</th>\n",
              "      <th>GQ_prc5_95</th>\n",
              "      <th>GQ_std_cycle_open</th>\n",
              "      <th>GQ_std_cycle_closed</th>\n",
              "      <th>GNE_mean</th>\n",
              "      <th>GNE_std</th>\n",
              "      <th>GNE_SNR_TKEO</th>\n",
              "      <th>...</th>\n",
              "      <th>tqwt_skewnessValue_dec_34</th>\n",
              "      <th>tqwt_skewnessValue_dec_35</th>\n",
              "      <th>tqwt_skewnessValue_dec_36</th>\n",
              "      <th>tqwt_kurtosisValue_dec_1</th>\n",
              "      <th>tqwt_kurtosisValue_dec_2</th>\n",
              "      <th>tqwt_kurtosisValue_dec_3</th>\n",
              "      <th>tqwt_kurtosisValue_dec_4</th>\n",
              "      <th>tqwt_kurtosisValue_dec_5</th>\n",
              "      <th>tqwt_kurtosisValue_dec_6</th>\n",
              "      <th>tqwt_kurtosisValue_dec_7</th>\n",
              "      <th>tqwt_kurtosisValue_dec_8</th>\n",
              "      <th>tqwt_kurtosisValue_dec_9</th>\n",
              "      <th>tqwt_kurtosisValue_dec_10</th>\n",
              "      <th>tqwt_kurtosisValue_dec_11</th>\n",
              "      <th>tqwt_kurtosisValue_dec_12</th>\n",
              "      <th>tqwt_kurtosisValue_dec_13</th>\n",
              "      <th>tqwt_kurtosisValue_dec_14</th>\n",
              "      <th>tqwt_kurtosisValue_dec_15</th>\n",
              "      <th>tqwt_kurtosisValue_dec_16</th>\n",
              "      <th>tqwt_kurtosisValue_dec_17</th>\n",
              "      <th>tqwt_kurtosisValue_dec_18</th>\n",
              "      <th>tqwt_kurtosisValue_dec_19</th>\n",
              "      <th>tqwt_kurtosisValue_dec_20</th>\n",
              "      <th>tqwt_kurtosisValue_dec_21</th>\n",
              "      <th>tqwt_kurtosisValue_dec_22</th>\n",
              "      <th>tqwt_kurtosisValue_dec_23</th>\n",
              "      <th>tqwt_kurtosisValue_dec_24</th>\n",
              "      <th>tqwt_kurtosisValue_dec_25</th>\n",
              "      <th>tqwt_kurtosisValue_dec_26</th>\n",
              "      <th>tqwt_kurtosisValue_dec_27</th>\n",
              "      <th>tqwt_kurtosisValue_dec_28</th>\n",
              "      <th>tqwt_kurtosisValue_dec_29</th>\n",
              "      <th>tqwt_kurtosisValue_dec_30</th>\n",
              "      <th>tqwt_kurtosisValue_dec_31</th>\n",
              "      <th>tqwt_kurtosisValue_dec_32</th>\n",
              "      <th>tqwt_kurtosisValue_dec_33</th>\n",
              "      <th>tqwt_kurtosisValue_dec_34</th>\n",
              "      <th>tqwt_kurtosisValue_dec_35</th>\n",
              "      <th>tqwt_kurtosisValue_dec_36</th>\n",
              "      <th>class</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>756</td>\n",
              "      <td>756</td>\n",
              "      <td>756</td>\n",
              "      <td>756</td>\n",
              "      <td>756</td>\n",
              "      <td>756</td>\n",
              "      <td>756</td>\n",
              "      <td>756</td>\n",
              "      <td>756</td>\n",
              "      <td>756</td>\n",
              "      <td>756</td>\n",
              "      <td>756</td>\n",
              "      <td>756</td>\n",
              "      <td>756</td>\n",
              "      <td>756</td>\n",
              "      <td>756</td>\n",
              "      <td>756</td>\n",
              "      <td>756</td>\n",
              "      <td>756</td>\n",
              "      <td>756</td>\n",
              "      <td>756</td>\n",
              "      <td>756</td>\n",
              "      <td>756</td>\n",
              "      <td>756</td>\n",
              "      <td>756</td>\n",
              "      <td>756</td>\n",
              "      <td>756</td>\n",
              "      <td>756</td>\n",
              "      <td>756</td>\n",
              "      <td>756</td>\n",
              "      <td>756</td>\n",
              "      <td>756</td>\n",
              "      <td>756</td>\n",
              "      <td>756</td>\n",
              "      <td>756</td>\n",
              "      <td>756</td>\n",
              "      <td>756</td>\n",
              "      <td>756</td>\n",
              "      <td>756</td>\n",
              "      <td>756</td>\n",
              "      <td>...</td>\n",
              "      <td>756</td>\n",
              "      <td>756</td>\n",
              "      <td>756</td>\n",
              "      <td>756</td>\n",
              "      <td>756</td>\n",
              "      <td>756</td>\n",
              "      <td>756</td>\n",
              "      <td>756</td>\n",
              "      <td>756</td>\n",
              "      <td>756</td>\n",
              "      <td>756</td>\n",
              "      <td>756</td>\n",
              "      <td>756</td>\n",
              "      <td>756</td>\n",
              "      <td>756</td>\n",
              "      <td>756</td>\n",
              "      <td>756</td>\n",
              "      <td>756</td>\n",
              "      <td>756</td>\n",
              "      <td>756</td>\n",
              "      <td>756</td>\n",
              "      <td>756</td>\n",
              "      <td>756</td>\n",
              "      <td>756</td>\n",
              "      <td>756</td>\n",
              "      <td>756</td>\n",
              "      <td>756</td>\n",
              "      <td>756</td>\n",
              "      <td>756</td>\n",
              "      <td>756</td>\n",
              "      <td>756</td>\n",
              "      <td>756</td>\n",
              "      <td>756</td>\n",
              "      <td>756</td>\n",
              "      <td>756</td>\n",
              "      <td>756</td>\n",
              "      <td>756</td>\n",
              "      <td>756</td>\n",
              "      <td>756</td>\n",
              "      <td>756</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>unique</th>\n",
              "      <td>252</td>\n",
              "      <td>2</td>\n",
              "      <td>740</td>\n",
              "      <td>745</td>\n",
              "      <td>748</td>\n",
              "      <td>315</td>\n",
              "      <td>319</td>\n",
              "      <td>755</td>\n",
              "      <td>646</td>\n",
              "      <td>358</td>\n",
              "      <td>543</td>\n",
              "      <td>183</td>\n",
              "      <td>244</td>\n",
              "      <td>327</td>\n",
              "      <td>734</td>\n",
              "      <td>547</td>\n",
              "      <td>710</td>\n",
              "      <td>709</td>\n",
              "      <td>722</td>\n",
              "      <td>741</td>\n",
              "      <td>747</td>\n",
              "      <td>748</td>\n",
              "      <td>745</td>\n",
              "      <td>755</td>\n",
              "      <td>755</td>\n",
              "      <td>755</td>\n",
              "      <td>755</td>\n",
              "      <td>755</td>\n",
              "      <td>755</td>\n",
              "      <td>755</td>\n",
              "      <td>755</td>\n",
              "      <td>755</td>\n",
              "      <td>755</td>\n",
              "      <td>754</td>\n",
              "      <td>356</td>\n",
              "      <td>753</td>\n",
              "      <td>380</td>\n",
              "      <td>743</td>\n",
              "      <td>750</td>\n",
              "      <td>735</td>\n",
              "      <td>...</td>\n",
              "      <td>753</td>\n",
              "      <td>754</td>\n",
              "      <td>754</td>\n",
              "      <td>755</td>\n",
              "      <td>754</td>\n",
              "      <td>754</td>\n",
              "      <td>752</td>\n",
              "      <td>754</td>\n",
              "      <td>754</td>\n",
              "      <td>751</td>\n",
              "      <td>752</td>\n",
              "      <td>753</td>\n",
              "      <td>752</td>\n",
              "      <td>754</td>\n",
              "      <td>751</td>\n",
              "      <td>749</td>\n",
              "      <td>741</td>\n",
              "      <td>744</td>\n",
              "      <td>742</td>\n",
              "      <td>742</td>\n",
              "      <td>740</td>\n",
              "      <td>738</td>\n",
              "      <td>746</td>\n",
              "      <td>735</td>\n",
              "      <td>739</td>\n",
              "      <td>732</td>\n",
              "      <td>728</td>\n",
              "      <td>723</td>\n",
              "      <td>735</td>\n",
              "      <td>750</td>\n",
              "      <td>749</td>\n",
              "      <td>755</td>\n",
              "      <td>752</td>\n",
              "      <td>753</td>\n",
              "      <td>749</td>\n",
              "      <td>752</td>\n",
              "      <td>753</td>\n",
              "      <td>753</td>\n",
              "      <td>754</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>top</th>\n",
              "      <td>13</td>\n",
              "      <td>1</td>\n",
              "      <td>0.82273</td>\n",
              "      <td>0.65177</td>\n",
              "      <td>0.70689</td>\n",
              "      <td>237</td>\n",
              "      <td>332</td>\n",
              "      <td>0.006004477</td>\n",
              "      <td>5.69E-05</td>\n",
              "      <td>0.00076</td>\n",
              "      <td>1.39E-05</td>\n",
              "      <td>0.00012</td>\n",
              "      <td>0.00032</td>\n",
              "      <td>0.00036</td>\n",
              "      <td>0.05227</td>\n",
              "      <td>0.376</td>\n",
              "      <td>0.04839</td>\n",
              "      <td>0.01948</td>\n",
              "      <td>0.02824</td>\n",
              "      <td>0.14516</td>\n",
              "      <td>0.998392</td>\n",
              "      <td>0.001613</td>\n",
              "      <td>18.344</td>\n",
              "      <td>80.44328048</td>\n",
              "      <td>85.61022901</td>\n",
              "      <td>82.73382546</td>\n",
              "      <td>769.6772481</td>\n",
              "      <td>1279.806544</td>\n",
              "      <td>3107.527459</td>\n",
              "      <td>3738.070756</td>\n",
              "      <td>76.4915472</td>\n",
              "      <td>117.5349021</td>\n",
              "      <td>192.2014234</td>\n",
              "      <td>299.1940012</td>\n",
              "      <td>1</td>\n",
              "      <td>6.9051</td>\n",
              "      <td>0</td>\n",
              "      <td>1.0221</td>\n",
              "      <td>0.10661</td>\n",
              "      <td>0.1442</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.091461</td>\n",
              "      <td>-2.7179</td>\n",
              "      <td>-8.9141</td>\n",
              "      <td>44.7366</td>\n",
              "      <td>15.7456</td>\n",
              "      <td>6.2387</td>\n",
              "      <td>9.1907</td>\n",
              "      <td>11.6205</td>\n",
              "      <td>4.1187</td>\n",
              "      <td>4.7652</td>\n",
              "      <td>3.7508</td>\n",
              "      <td>4.3825</td>\n",
              "      <td>4.5152</td>\n",
              "      <td>4.3659</td>\n",
              "      <td>3.7173</td>\n",
              "      <td>2.6395</td>\n",
              "      <td>2.4171</td>\n",
              "      <td>2.9004</td>\n",
              "      <td>2.2731</td>\n",
              "      <td>2.5803</td>\n",
              "      <td>2.1907</td>\n",
              "      <td>2.4863</td>\n",
              "      <td>2.9735</td>\n",
              "      <td>1.5932</td>\n",
              "      <td>1.6444</td>\n",
              "      <td>1.7484</td>\n",
              "      <td>2.3105</td>\n",
              "      <td>1.5842</td>\n",
              "      <td>1.6337</td>\n",
              "      <td>1.613</td>\n",
              "      <td>3.4835</td>\n",
              "      <td>4.0251</td>\n",
              "      <td>4.4021</td>\n",
              "      <td>4.2105</td>\n",
              "      <td>2.3974</td>\n",
              "      <td>3.1144</td>\n",
              "      <td>6.2712</td>\n",
              "      <td>4.2391</td>\n",
              "      <td>10.0693</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>freq</th>\n",
              "      <td>3</td>\n",
              "      <td>390</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>9</td>\n",
              "      <td>8</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>9</td>\n",
              "      <td>10</td>\n",
              "      <td>24</td>\n",
              "      <td>14</td>\n",
              "      <td>11</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>385</td>\n",
              "      <td>2</td>\n",
              "      <td>377</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>...</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>564</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>4 rows Ã— 755 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "0        id gender  ... tqwt_kurtosisValue_dec_36 class\n",
              "count   756    756  ...                       756   756\n",
              "unique  252      2  ...                       754     2\n",
              "top      13      1  ...                   10.0693     1\n",
              "freq      3    390  ...                         2   564\n",
              "\n",
              "[4 rows x 755 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a9FvWGzGrurv",
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 142
        },
        "outputId": "19ed5697-af29-444b-9f53-bd55bcb2b67f"
      },
      "source": [
        "pd_speech_features['patient/healthy count'] = 1\n",
        "pd_speech_features.groupby('class').sum()/3"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>patient/healthy count</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>class</th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>64.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>188.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "0      patient/healthy count\n",
              "class                       \n",
              "0                       64.0\n",
              "1                      188.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L8ppw8fnrur1",
        "trusted": true
      },
      "source": [
        "pd_speech_features = pd_speech_features.drop(['patient/healthy count'], axis = 1)  #756x755"
      ],
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BY5oJXz9rur4",
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "79103557-cd62-4711-bc9b-32516306e7fd"
      },
      "source": [
        "pd_speech_features =  pd_speech_features.astype(float) #per default all floats \n",
        "pd_speech_features[['id', 'numPulses', 'numPeriodsPulses']] = pd_speech_features[['id', 'numPulses', 'numPeriodsPulses']].astype(int) #ints\n",
        "pd_speech_features[['gender', 'class']] = pd_speech_features[['gender', 'class']].astype('category') #categoricals\n",
        "pd_speech_features.dtypes"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0\n",
              "id                              int64\n",
              "gender                       category\n",
              "PPE                           float64\n",
              "DFA                           float64\n",
              "RPDE                          float64\n",
              "                               ...   \n",
              "tqwt_kurtosisValue_dec_33     float64\n",
              "tqwt_kurtosisValue_dec_34     float64\n",
              "tqwt_kurtosisValue_dec_35     float64\n",
              "tqwt_kurtosisValue_dec_36     float64\n",
              "class                        category\n",
              "Length: 755, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WBwpHHK3rur9",
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 253
        },
        "outputId": "7d0fdc1d-9a54-4461-f981-5469232dbb19"
      },
      "source": [
        "pd_speech_features_no_tqwt = pd_speech_features[pd_speech_features.columns[1: -433]]\n",
        "pd_speech_features_no_tqwt.head()"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>gender</th>\n",
              "      <th>PPE</th>\n",
              "      <th>DFA</th>\n",
              "      <th>RPDE</th>\n",
              "      <th>numPulses</th>\n",
              "      <th>numPeriodsPulses</th>\n",
              "      <th>meanPeriodPulses</th>\n",
              "      <th>stdDevPeriodPulses</th>\n",
              "      <th>locPctJitter</th>\n",
              "      <th>locAbsJitter</th>\n",
              "      <th>rapJitter</th>\n",
              "      <th>ppq5Jitter</th>\n",
              "      <th>ddpJitter</th>\n",
              "      <th>locShimmer</th>\n",
              "      <th>locDbShimmer</th>\n",
              "      <th>apq3Shimmer</th>\n",
              "      <th>apq5Shimmer</th>\n",
              "      <th>apq11Shimmer</th>\n",
              "      <th>ddaShimmer</th>\n",
              "      <th>meanAutoCorrHarmonicity</th>\n",
              "      <th>meanNoiseToHarmHarmonicity</th>\n",
              "      <th>meanHarmToNoiseHarmonicity</th>\n",
              "      <th>minIntensity</th>\n",
              "      <th>maxIntensity</th>\n",
              "      <th>meanIntensity</th>\n",
              "      <th>f1</th>\n",
              "      <th>f2</th>\n",
              "      <th>f3</th>\n",
              "      <th>f4</th>\n",
              "      <th>b1</th>\n",
              "      <th>b2</th>\n",
              "      <th>b3</th>\n",
              "      <th>b4</th>\n",
              "      <th>GQ_prc5_95</th>\n",
              "      <th>GQ_std_cycle_open</th>\n",
              "      <th>GQ_std_cycle_closed</th>\n",
              "      <th>GNE_mean</th>\n",
              "      <th>GNE_std</th>\n",
              "      <th>GNE_SNR_TKEO</th>\n",
              "      <th>GNE_SNR_SEO</th>\n",
              "      <th>...</th>\n",
              "      <th>app_LT_entropy_shannon_1_coef</th>\n",
              "      <th>app_LT_entropy_shannon_2_coef</th>\n",
              "      <th>app_LT_entropy_shannon_3_coef</th>\n",
              "      <th>app_LT_entropy_shannon_4_coef</th>\n",
              "      <th>app_LT_entropy_shannon_5_coef</th>\n",
              "      <th>app_LT_entropy_shannon_6_coef</th>\n",
              "      <th>app_LT_entropy_shannon_7_coef</th>\n",
              "      <th>app_LT_entropy_shannon_8_coef</th>\n",
              "      <th>app_LT_entropy_shannon_9_coef</th>\n",
              "      <th>app_LT_entropy_shannon_10_coef</th>\n",
              "      <th>app_LT_entropy_log_1_coef</th>\n",
              "      <th>app_LT_entropy_log_2_coef</th>\n",
              "      <th>app_LT_entropy_log_3_coef</th>\n",
              "      <th>app_LT_entropy_log_4_coef</th>\n",
              "      <th>app_LT_entropy_log_5_coef</th>\n",
              "      <th>app_LT_entropy_log_6_coef</th>\n",
              "      <th>app_LT_entropy_log_7_coef</th>\n",
              "      <th>app_LT_entropy_log_8_coef</th>\n",
              "      <th>app_LT_entropy_log_9_coef</th>\n",
              "      <th>app_LT_entropy_log_10_coef</th>\n",
              "      <th>app_LT_TKEO_mean_1_coef</th>\n",
              "      <th>app_LT_TKEO_mean_2_coef</th>\n",
              "      <th>app_LT_TKEO_mean_3_coef</th>\n",
              "      <th>app_LT_TKEO_mean_4_coef</th>\n",
              "      <th>app_LT_TKEO_mean_5_coef</th>\n",
              "      <th>app_LT_TKEO_mean_6_coef</th>\n",
              "      <th>app_LT_TKEO_mean_7_coef</th>\n",
              "      <th>app_LT_TKEO_mean_8_coef</th>\n",
              "      <th>app_LT_TKEO_mean_9_coef</th>\n",
              "      <th>app_LT_TKEO_mean_10_coef</th>\n",
              "      <th>app_LT_TKEO_std_1_coef</th>\n",
              "      <th>app_LT_TKEO_std_2_coef</th>\n",
              "      <th>app_LT_TKEO_std_3_coef</th>\n",
              "      <th>app_LT_TKEO_std_4_coef</th>\n",
              "      <th>app_LT_TKEO_std_5_coef</th>\n",
              "      <th>app_LT_TKEO_std_6_coef</th>\n",
              "      <th>app_LT_TKEO_std_7_coef</th>\n",
              "      <th>app_LT_TKEO_std_8_coef</th>\n",
              "      <th>app_LT_TKEO_std_9_coef</th>\n",
              "      <th>app_LT_TKEO_std_10_coef</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1.0</td>\n",
              "      <td>0.85247</td>\n",
              "      <td>0.71826</td>\n",
              "      <td>0.57227</td>\n",
              "      <td>240</td>\n",
              "      <td>239</td>\n",
              "      <td>0.008064</td>\n",
              "      <td>0.000087</td>\n",
              "      <td>0.00218</td>\n",
              "      <td>0.000018</td>\n",
              "      <td>0.00067</td>\n",
              "      <td>0.00129</td>\n",
              "      <td>0.00200</td>\n",
              "      <td>0.05883</td>\n",
              "      <td>0.517</td>\n",
              "      <td>0.03011</td>\n",
              "      <td>0.03496</td>\n",
              "      <td>0.04828</td>\n",
              "      <td>0.09034</td>\n",
              "      <td>0.970805</td>\n",
              "      <td>0.036223</td>\n",
              "      <td>18.995</td>\n",
              "      <td>69.997496</td>\n",
              "      <td>76.088046</td>\n",
              "      <td>72.465512</td>\n",
              "      <td>539.342735</td>\n",
              "      <td>1031.849040</td>\n",
              "      <td>2447.162183</td>\n",
              "      <td>3655.054806</td>\n",
              "      <td>101.092218</td>\n",
              "      <td>83.147440</td>\n",
              "      <td>255.214830</td>\n",
              "      <td>396.643631</td>\n",
              "      <td>0.77778</td>\n",
              "      <td>11.7245</td>\n",
              "      <td>2.8277</td>\n",
              "      <td>1.17300</td>\n",
              "      <td>0.26512</td>\n",
              "      <td>0.083127</td>\n",
              "      <td>1200445.612</td>\n",
              "      <td>...</td>\n",
              "      <td>-19278.0371</td>\n",
              "      <td>-25711.8622</td>\n",
              "      <td>-36938.1370</td>\n",
              "      <td>-57264.6625</td>\n",
              "      <td>-98433.1856</td>\n",
              "      <td>-184901.7535</td>\n",
              "      <td>-381059.3510</td>\n",
              "      <td>-776445.2329</td>\n",
              "      <td>-1676725.978</td>\n",
              "      <td>-3601122.613</td>\n",
              "      <td>414.6434</td>\n",
              "      <td>276.4850</td>\n",
              "      <td>198.5803</td>\n",
              "      <td>153.8978</td>\n",
              "      <td>132.2489</td>\n",
              "      <td>124.1971</td>\n",
              "      <td>127.9812</td>\n",
              "      <td>130.3804</td>\n",
              "      <td>140.7776</td>\n",
              "      <td>151.1748</td>\n",
              "      <td>0.86121</td>\n",
              "      <td>3.0487</td>\n",
              "      <td>9.7825</td>\n",
              "      <td>28.5949</td>\n",
              "      <td>74.3411</td>\n",
              "      <td>174.9214</td>\n",
              "      <td>371.7296</td>\n",
              "      <td>793.0680</td>\n",
              "      <td>1586.1824</td>\n",
              "      <td>3173.0448</td>\n",
              "      <td>6.2990</td>\n",
              "      <td>16.7003</td>\n",
              "      <td>42.0762</td>\n",
              "      <td>101.0889</td>\n",
              "      <td>228.8489</td>\n",
              "      <td>493.8563</td>\n",
              "      <td>1015.7707</td>\n",
              "      <td>2091.9460</td>\n",
              "      <td>4188.2456</td>\n",
              "      <td>8373.9278</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1.0</td>\n",
              "      <td>0.76686</td>\n",
              "      <td>0.69481</td>\n",
              "      <td>0.53966</td>\n",
              "      <td>234</td>\n",
              "      <td>233</td>\n",
              "      <td>0.008258</td>\n",
              "      <td>0.000073</td>\n",
              "      <td>0.00195</td>\n",
              "      <td>0.000016</td>\n",
              "      <td>0.00052</td>\n",
              "      <td>0.00112</td>\n",
              "      <td>0.00157</td>\n",
              "      <td>0.05516</td>\n",
              "      <td>0.502</td>\n",
              "      <td>0.02320</td>\n",
              "      <td>0.03675</td>\n",
              "      <td>0.06195</td>\n",
              "      <td>0.06961</td>\n",
              "      <td>0.984322</td>\n",
              "      <td>0.017974</td>\n",
              "      <td>21.497</td>\n",
              "      <td>67.415903</td>\n",
              "      <td>73.046374</td>\n",
              "      <td>71.528945</td>\n",
              "      <td>564.363615</td>\n",
              "      <td>1016.367294</td>\n",
              "      <td>2383.565201</td>\n",
              "      <td>3498.681572</td>\n",
              "      <td>58.465428</td>\n",
              "      <td>86.487292</td>\n",
              "      <td>248.357127</td>\n",
              "      <td>218.229722</td>\n",
              "      <td>0.81250</td>\n",
              "      <td>13.8284</td>\n",
              "      <td>2.8908</td>\n",
              "      <td>1.02210</td>\n",
              "      <td>0.22004</td>\n",
              "      <td>0.127410</td>\n",
              "      <td>1298455.445</td>\n",
              "      <td>...</td>\n",
              "      <td>-19028.6532</td>\n",
              "      <td>-25392.0069</td>\n",
              "      <td>-36496.8101</td>\n",
              "      <td>-56599.2563</td>\n",
              "      <td>-97324.8830</td>\n",
              "      <td>-182880.5032</td>\n",
              "      <td>-376979.9939</td>\n",
              "      <td>-768230.2335</td>\n",
              "      <td>-1659120.382</td>\n",
              "      <td>-3563560.603</td>\n",
              "      <td>413.5284</td>\n",
              "      <td>275.8597</td>\n",
              "      <td>198.1971</td>\n",
              "      <td>153.6379</td>\n",
              "      <td>132.0522</td>\n",
              "      <td>124.0327</td>\n",
              "      <td>127.8282</td>\n",
              "      <td>130.2373</td>\n",
              "      <td>140.6345</td>\n",
              "      <td>151.0317</td>\n",
              "      <td>0.85289</td>\n",
              "      <td>3.0213</td>\n",
              "      <td>9.6956</td>\n",
              "      <td>28.3506</td>\n",
              "      <td>73.7185</td>\n",
              "      <td>173.4666</td>\n",
              "      <td>368.5705</td>\n",
              "      <td>786.0763</td>\n",
              "      <td>1572.1837</td>\n",
              "      <td>3144.4525</td>\n",
              "      <td>6.2381</td>\n",
              "      <td>16.5376</td>\n",
              "      <td>41.7306</td>\n",
              "      <td>100.0918</td>\n",
              "      <td>226.9019</td>\n",
              "      <td>489.9169</td>\n",
              "      <td>1006.3702</td>\n",
              "      <td>2074.4541</td>\n",
              "      <td>4148.9889</td>\n",
              "      <td>8298.1606</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1.0</td>\n",
              "      <td>0.85083</td>\n",
              "      <td>0.67604</td>\n",
              "      <td>0.58982</td>\n",
              "      <td>232</td>\n",
              "      <td>231</td>\n",
              "      <td>0.008340</td>\n",
              "      <td>0.000060</td>\n",
              "      <td>0.00176</td>\n",
              "      <td>0.000015</td>\n",
              "      <td>0.00057</td>\n",
              "      <td>0.00111</td>\n",
              "      <td>0.00171</td>\n",
              "      <td>0.09902</td>\n",
              "      <td>0.897</td>\n",
              "      <td>0.05094</td>\n",
              "      <td>0.06497</td>\n",
              "      <td>0.07772</td>\n",
              "      <td>0.15282</td>\n",
              "      <td>0.974846</td>\n",
              "      <td>0.026313</td>\n",
              "      <td>17.651</td>\n",
              "      <td>62.661706</td>\n",
              "      <td>71.633549</td>\n",
              "      <td>68.086583</td>\n",
              "      <td>548.444604</td>\n",
              "      <td>1032.406341</td>\n",
              "      <td>2357.826954</td>\n",
              "      <td>3678.128717</td>\n",
              "      <td>160.387771</td>\n",
              "      <td>54.685168</td>\n",
              "      <td>151.694847</td>\n",
              "      <td>84.240339</td>\n",
              "      <td>0.81818</td>\n",
              "      <td>26.9273</td>\n",
              "      <td>2.6975</td>\n",
              "      <td>0.84951</td>\n",
              "      <td>0.15756</td>\n",
              "      <td>0.116890</td>\n",
              "      <td>1272869.841</td>\n",
              "      <td>...</td>\n",
              "      <td>-18926.4578</td>\n",
              "      <td>-25253.6144</td>\n",
              "      <td>-36288.7542</td>\n",
              "      <td>-56258.4752</td>\n",
              "      <td>-96708.1119</td>\n",
              "      <td>-181663.4768</td>\n",
              "      <td>-374463.8517</td>\n",
              "      <td>-763056.6385</td>\n",
              "      <td>-1648032.246</td>\n",
              "      <td>-3539902.400</td>\n",
              "      <td>413.0675</td>\n",
              "      <td>275.5868</td>\n",
              "      <td>198.0149</td>\n",
              "      <td>153.5036</td>\n",
              "      <td>131.9418</td>\n",
              "      <td>123.9329</td>\n",
              "      <td>127.7329</td>\n",
              "      <td>130.1464</td>\n",
              "      <td>140.5436</td>\n",
              "      <td>150.9408</td>\n",
              "      <td>0.84977</td>\n",
              "      <td>3.0062</td>\n",
              "      <td>9.6470</td>\n",
              "      <td>28.2014</td>\n",
              "      <td>73.3297</td>\n",
              "      <td>172.5204</td>\n",
              "      <td>366.5936</td>\n",
              "      <td>782.0604</td>\n",
              "      <td>1564.1081</td>\n",
              "      <td>3128.0295</td>\n",
              "      <td>6.2163</td>\n",
              "      <td>16.4817</td>\n",
              "      <td>41.4869</td>\n",
              "      <td>99.6154</td>\n",
              "      <td>225.7803</td>\n",
              "      <td>486.9865</td>\n",
              "      <td>1001.7348</td>\n",
              "      <td>2064.1067</td>\n",
              "      <td>4127.0967</td>\n",
              "      <td>8254.7868</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.41121</td>\n",
              "      <td>0.79672</td>\n",
              "      <td>0.59257</td>\n",
              "      <td>178</td>\n",
              "      <td>177</td>\n",
              "      <td>0.010858</td>\n",
              "      <td>0.000183</td>\n",
              "      <td>0.00419</td>\n",
              "      <td>0.000046</td>\n",
              "      <td>0.00149</td>\n",
              "      <td>0.00268</td>\n",
              "      <td>0.00446</td>\n",
              "      <td>0.05451</td>\n",
              "      <td>0.527</td>\n",
              "      <td>0.02395</td>\n",
              "      <td>0.02857</td>\n",
              "      <td>0.04462</td>\n",
              "      <td>0.07185</td>\n",
              "      <td>0.968343</td>\n",
              "      <td>0.042003</td>\n",
              "      <td>19.865</td>\n",
              "      <td>76.306989</td>\n",
              "      <td>81.000749</td>\n",
              "      <td>79.190593</td>\n",
              "      <td>819.529588</td>\n",
              "      <td>1201.813897</td>\n",
              "      <td>3154.035654</td>\n",
              "      <td>4122.163933</td>\n",
              "      <td>238.667052</td>\n",
              "      <td>191.984916</td>\n",
              "      <td>573.752909</td>\n",
              "      <td>526.147599</td>\n",
              "      <td>0.98548</td>\n",
              "      <td>139.5744</td>\n",
              "      <td>1.6961</td>\n",
              "      <td>0.83405</td>\n",
              "      <td>0.17295</td>\n",
              "      <td>0.147370</td>\n",
              "      <td>1932289.206</td>\n",
              "      <td>...</td>\n",
              "      <td>-19352.0891</td>\n",
              "      <td>-25452.5218</td>\n",
              "      <td>-35824.8476</td>\n",
              "      <td>-54370.9290</td>\n",
              "      <td>-91686.1704</td>\n",
              "      <td>-169639.1274</td>\n",
              "      <td>-347128.0292</td>\n",
              "      <td>-704471.7514</td>\n",
              "      <td>-1522358.498</td>\n",
              "      <td>-3271399.011</td>\n",
              "      <td>413.6380</td>\n",
              "      <td>275.3259</td>\n",
              "      <td>197.2795</td>\n",
              "      <td>152.5940</td>\n",
              "      <td>130.9480</td>\n",
              "      <td>122.8786</td>\n",
              "      <td>126.6411</td>\n",
              "      <td>129.0689</td>\n",
              "      <td>139.4666</td>\n",
              "      <td>149.8649</td>\n",
              "      <td>0.88367</td>\n",
              "      <td>2.9398</td>\n",
              "      <td>9.0446</td>\n",
              "      <td>26.2220</td>\n",
              "      <td>67.4885</td>\n",
              "      <td>158.1634</td>\n",
              "      <td>336.9109</td>\n",
              "      <td>724.9443</td>\n",
              "      <td>1448.7625</td>\n",
              "      <td>2913.3877</td>\n",
              "      <td>6.7833</td>\n",
              "      <td>16.8216</td>\n",
              "      <td>41.3157</td>\n",
              "      <td>94.4579</td>\n",
              "      <td>211.1565</td>\n",
              "      <td>443.3447</td>\n",
              "      <td>955.8128</td>\n",
              "      <td>1890.1299</td>\n",
              "      <td>3910.7029</td>\n",
              "      <td>7698.9389</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.32790</td>\n",
              "      <td>0.79782</td>\n",
              "      <td>0.53028</td>\n",
              "      <td>236</td>\n",
              "      <td>235</td>\n",
              "      <td>0.008162</td>\n",
              "      <td>0.002669</td>\n",
              "      <td>0.00535</td>\n",
              "      <td>0.000044</td>\n",
              "      <td>0.00166</td>\n",
              "      <td>0.00227</td>\n",
              "      <td>0.00499</td>\n",
              "      <td>0.05610</td>\n",
              "      <td>0.497</td>\n",
              "      <td>0.02909</td>\n",
              "      <td>0.03327</td>\n",
              "      <td>0.05278</td>\n",
              "      <td>0.08728</td>\n",
              "      <td>0.975754</td>\n",
              "      <td>0.027139</td>\n",
              "      <td>19.557</td>\n",
              "      <td>76.645686</td>\n",
              "      <td>80.937258</td>\n",
              "      <td>79.183495</td>\n",
              "      <td>846.796144</td>\n",
              "      <td>1215.346469</td>\n",
              "      <td>3201.513132</td>\n",
              "      <td>4085.456839</td>\n",
              "      <td>402.216738</td>\n",
              "      <td>210.061394</td>\n",
              "      <td>203.637106</td>\n",
              "      <td>384.611697</td>\n",
              "      <td>0.97847</td>\n",
              "      <td>102.0549</td>\n",
              "      <td>15.4045</td>\n",
              "      <td>0.83556</td>\n",
              "      <td>0.16210</td>\n",
              "      <td>0.151990</td>\n",
              "      <td>1861807.802</td>\n",
              "      <td>...</td>\n",
              "      <td>-21066.0878</td>\n",
              "      <td>-27472.1915</td>\n",
              "      <td>-38399.3353</td>\n",
              "      <td>-57514.4436</td>\n",
              "      <td>-95740.4317</td>\n",
              "      <td>-176007.1589</td>\n",
              "      <td>-359900.4024</td>\n",
              "      <td>-725678.1466</td>\n",
              "      <td>-1567705.131</td>\n",
              "      <td>-3367165.736</td>\n",
              "      <td>421.1396</td>\n",
              "      <td>279.1703</td>\n",
              "      <td>199.4350</td>\n",
              "      <td>153.7788</td>\n",
              "      <td>131.6258</td>\n",
              "      <td>123.3566</td>\n",
              "      <td>127.0837</td>\n",
              "      <td>129.4029</td>\n",
              "      <td>139.8011</td>\n",
              "      <td>150.2018</td>\n",
              "      <td>0.94619</td>\n",
              "      <td>3.2838</td>\n",
              "      <td>10.5898</td>\n",
              "      <td>30.3224</td>\n",
              "      <td>78.2530</td>\n",
              "      <td>182.9658</td>\n",
              "      <td>390.1740</td>\n",
              "      <td>827.9359</td>\n",
              "      <td>1653.5676</td>\n",
              "      <td>3265.5292</td>\n",
              "      <td>6.9366</td>\n",
              "      <td>18.3595</td>\n",
              "      <td>46.2704</td>\n",
              "      <td>108.6792</td>\n",
              "      <td>244.0607</td>\n",
              "      <td>541.2414</td>\n",
              "      <td>1057.2566</td>\n",
              "      <td>2242.5460</td>\n",
              "      <td>4297.4639</td>\n",
              "      <td>8645.2845</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows Ã— 321 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "0 gender      PPE  ...  app_LT_TKEO_std_9_coef  app_LT_TKEO_std_10_coef\n",
              "1    1.0  0.85247  ...               4188.2456                8373.9278\n",
              "2    1.0  0.76686  ...               4148.9889                8298.1606\n",
              "3    1.0  0.85083  ...               4127.0967                8254.7868\n",
              "4    0.0  0.41121  ...               3910.7029                7698.9389\n",
              "5    0.0  0.32790  ...               4297.4639                8645.2845\n",
              "\n",
              "[5 rows x 321 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uQ8SbaiTCRqP",
        "trusted": true
      },
      "source": [
        "train_df=pd_speech_features\n",
        "train_df_tqwt=pd_speech_features_no_tqwt\n",
        "\n",
        "y_train = train_df['class']\n",
        "y_train = np.array(y_train.values, dtype = 'int')\n",
        "x_train = train_df.drop(['class','id'], axis = 1) \n",
        "\n",
        "x_train = x_train.values\n",
        "#y_validation = validation_df['class']\n",
        "#y_validation = np.array(y_validation.values, dtype = 'int')\n",
        "#x_validation = validation_df.drop(['class'], axis = 1) "
      ],
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gOPAWkkDH6c6",
        "outputId": "eba1931e-b7bc-4dc2-d950-0de5209ad883"
      },
      "source": [
        "features=train_df.columns.values.tolist()\n",
        "features.remove('id')\n",
        "features.remove('class')\n",
        "print(len(features))"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "753\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QkotwMAfqW6W",
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4f800456-2260-4735-f826-f72bdabb5e91"
      },
      "source": [
        "print(x_train.shape)\n",
        "print(y_train.shape)\n"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(756, 753)\n",
            "(756,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tteEg1QRItRm",
        "outputId": "ec8ed7ce-6afb-4a6e-c7ff-8abf1d4c87a4"
      },
      "source": [
        "!pip install --upgrade scikit-learn"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already up-to-date: scikit-learn in /usr/local/lib/python3.7/dist-packages (0.24.1)\n",
            "Requirement already satisfied, skipping upgrade: numpy>=1.13.3 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (1.19.5)\n",
            "Requirement already satisfied, skipping upgrade: scipy>=0.19.1 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (1.4.1)\n",
            "Requirement already satisfied, skipping upgrade: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (1.0.1)\n",
            "Requirement already satisfied, skipping upgrade: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (2.1.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xZgiEXM-XTxv",
        "trusted": true
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "sc = StandardScaler()\n",
        "\n",
        "x_train= sc.fit_transform(x_train)"
      ],
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "5v1oVAU-H6c7"
      },
      "source": [
        "from sklearn.feature_selection import SequentialFeatureSelector\n",
        "sfs_9 = SequentialFeatureSelector(LogisticRegression(random_state=10),n_features_to_select=9, n_jobs=-1)\n",
        "\n",
        "sfs_9=sfs_9.fit(x_train, y_train)"
      ],
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gWZL5YClH6c9",
        "outputId": "438a0375-dc68-4c22-db5d-1fe8a1b2bd67"
      },
      "source": [
        "x_train_transformed_9_features=sfs_9.transform(x_train)\n",
        "x_train_transformed_9_features.shape"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(756, 9)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bIgtx3FbTc4t"
      },
      "source": [
        "# LSTM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lPtdGmwVmeEI",
        "trusted": true
      },
      "source": [
        "from sklearn.model_selection import LeaveOneOut, KFold\n",
        "from sklearn.metrics import accuracy_score, f1_score, classification_report"
      ],
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1McWKloGrusQ",
        "trusted": true
      },
      "source": [
        "from sklearn import svm \n",
        "from sklearn.model_selection import cross_val_score, GridSearchCV\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "\n",
        "param_grid = {'C': [0.1, 1, 10, 100, 1000],  \n",
        "              'gamma': [1, 0.1, 0.01, 0.001, 0.0001], \n",
        "              'kernel': ['rbf']}  "
      ],
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VOlnHs_wprt5",
        "trusted": true
      },
      "source": [
        "k_fold=KFold(10,True,10)"
      ],
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "eXuxV9-3H6c_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ca60f5fb-b4a8-44b2-b68a-2e0e4f837121"
      },
      "source": [
        "import torch.nn as nn\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "torch.manual_seed(10)"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7f1e12fdcaf0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "Glj55X8yH6c_"
      },
      "source": [
        "m=nn.Sequential(nn.Linear(18,18), nn.ReLU(),nn.Linear(18,18),nn.ReLU())\n",
        "\n",
        "class Model(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.l1=nn.Linear(9,18)\n",
        "        self.l2=nn.Linear(18,18)\n",
        "        self.l3=nn.Linear(18,18)\n",
        "        self.ll=m\n",
        "        self.l4=nn.Linear(18,1)\n",
        "        \n",
        "    def forward(self,x):\n",
        "        x=self.l1(x)\n",
        "        x=F.relu(F.dropout(x,0.200))\n",
        "        x1=self.l2(x)\n",
        "        x2=F.relu(F.dropout(x,0.20))\n",
        "        x=self.l3(x)\n",
        "        x=self.ll((x+x1)/2.00)\n",
        "        x=F.relu(F.dropout(x,.200))\n",
        "        x3=self.ll((x))\n",
        "        x=self.l3(x)\n",
        "        x=F.relu(F.dropout((x+x1)/2.0,0.000))\n",
        "        x=self.l4(x)\n",
        "        \n",
        "        return x\n",
        "        \n"
      ],
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "aGkB3k10H6c_"
      },
      "source": [
        "#x=torch.from_numpy(x_train_transformed_9_features[:600]).float()\n",
        "#x=torch.from_numpy(x_train[:600]).float()\n",
        "#y=torch.from_numpy(y_train[:600]).float()\n",
        "\n",
        "def train(model,optimizer, scheduler, criterion, x,y):\n",
        "    \n",
        "    losses=[]\n",
        "    \n",
        "    for i in range(100):\n",
        "        output=model(x)\n",
        "        loss=criterion(output.view(y.shape[0]),y)\n",
        "    \n",
        "        losses.append(loss)\n",
        "    \n",
        "        #if (i%100==0):\n",
        "         #   print(i,' ',loss.item())\n",
        "    \n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "    \n",
        "    # training accuracy\n",
        "    with torch.no_grad():\n",
        "        preds=torch.sigmoid(model(x)).round()\n",
        "        print('\\ntraining report: \\n',classification_report(preds.view(preds.shape[0]),y))\n",
        "        \n",
        "    return model, losses\n",
        "\n",
        "#model, losses= train(model,optimizer, scheduler, criterion,x,y)"
      ],
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "5Za9Fg9eH6dA"
      },
      "source": [
        "def test(model, tstx, tsty):\n",
        "    with torch.no_grad():\n",
        "    \n",
        "        preds=torch.sigmoid(model(tstx)).round()\n",
        "        print('testing report:')\n",
        "        print(classification_report(preds.view(preds.shape[0]), tsty))\n",
        "        print(torch.sigmoid(preds).round().shape)"
      ],
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "K-GCeSQGH6dA"
      },
      "source": [
        "#tstx=torch.from_numpy(x_train_transformed_9_features[600:]).float()\n",
        "#tstx=torch.from_numpy(x_train[600:]).float()\n",
        "tsty=torch.from_numpy(y_train[600:]).float()\n",
        "\n",
        "#test(model,tstx,tsty)"
      ],
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "PnfCcJo9H6dB"
      },
      "source": [
        "def run_network(model, trainx,trainy, testx, testy):\n",
        "    model=Model()\n",
        "    criterion = nn.BCEWithLogitsLoss()\n",
        "    optimizer=torch.optim.SGD(model.parameters(),lr=0.8,momentum=0.99, nesterov=True)\n",
        "    #optim=torch.optim.Adam(model.parameters(),lr=0.1)\n",
        "    scheduler=torch.optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.1)\n",
        "    model, losses= train(model,optimizer, scheduler, criterion,trainx, trainy)\n",
        "    test(model, testx, testy)\n",
        "    \n",
        "    return model, losses"
      ],
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Hh_j13-H6dB",
        "outputId": "0af71d64-a8c9-4732-f3a1-375d9022c7eb"
      },
      "source": [
        "fold=0\n",
        "losses=[[] for i in range(10)]\n",
        "\n",
        "for trids, tstids in k_fold.split(x_train_transformed_9_features):\n",
        "    trainx, testx=x_train_transformed_9_features[trids,:], x_train_transformed_9_features[tstids,:]\n",
        "    #trainx, testx=x_train[trids,:], x_train[tstids,:]\n",
        "    trainy, testy=y_train[trids], y_train[tstids]\n",
        "    \n",
        "    trainx=torch.from_numpy(trainx).float()\n",
        "    trainy=torch.from_numpy(trainy).float()\n",
        "    testx=torch.from_numpy(testx).float()\n",
        "    testy=torch.from_numpy(testy).float()\n",
        "    \n",
        "    print('fold: ',fold)\n",
        "    \n",
        "    model=Model()\n",
        "    \n",
        "    _,losses[fold]=run_network(model, trainx,trainy, testx,testy)\n",
        "    \n",
        "    fold=fold+1\n",
        "\n",
        "\n"
      ],
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fold:  0\n",
            "\n",
            "training report: \n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.52      0.84      0.64       110\n",
            "         1.0       0.96      0.85      0.90       570\n",
            "\n",
            "    accuracy                           0.85       680\n",
            "   macro avg       0.74      0.84      0.77       680\n",
            "weighted avg       0.89      0.85      0.86       680\n",
            "\n",
            "testing report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.36      0.83      0.50         6\n",
            "         1.0       0.98      0.87      0.92        70\n",
            "\n",
            "    accuracy                           0.87        76\n",
            "   macro avg       0.67      0.85      0.71        76\n",
            "weighted avg       0.93      0.87      0.89        76\n",
            "\n",
            "torch.Size([76, 1])\n",
            "fold:  1\n",
            "\n",
            "training report: \n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.51      0.83      0.63       105\n",
            "         1.0       0.96      0.85      0.90       575\n",
            "\n",
            "    accuracy                           0.85       680\n",
            "   macro avg       0.74      0.84      0.77       680\n",
            "weighted avg       0.89      0.85      0.86       680\n",
            "\n",
            "testing report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.45      0.69      0.55        13\n",
            "         1.0       0.93      0.83      0.87        63\n",
            "\n",
            "    accuracy                           0.80        76\n",
            "   macro avg       0.69      0.76      0.71        76\n",
            "weighted avg       0.85      0.80      0.82        76\n",
            "\n",
            "torch.Size([76, 1])\n",
            "fold:  2\n",
            "\n",
            "training report: \n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.48      0.82      0.61       102\n",
            "         1.0       0.96      0.84      0.90       578\n",
            "\n",
            "    accuracy                           0.84       680\n",
            "   macro avg       0.72      0.83      0.75       680\n",
            "weighted avg       0.89      0.84      0.86       680\n",
            "\n",
            "testing report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.61      0.92      0.73        12\n",
            "         1.0       0.98      0.89      0.93        64\n",
            "\n",
            "    accuracy                           0.89        76\n",
            "   macro avg       0.80      0.90      0.83        76\n",
            "weighted avg       0.92      0.89      0.90        76\n",
            "\n",
            "torch.Size([76, 1])\n",
            "fold:  3\n",
            "\n",
            "training report: \n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.47      0.80      0.59       103\n",
            "         1.0       0.96      0.84      0.90       577\n",
            "\n",
            "    accuracy                           0.83       680\n",
            "   macro avg       0.71      0.82      0.74       680\n",
            "weighted avg       0.88      0.83      0.85       680\n",
            "\n",
            "testing report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.44      0.80      0.57        10\n",
            "         1.0       0.97      0.85      0.90        66\n",
            "\n",
            "    accuracy                           0.84        76\n",
            "   macro avg       0.70      0.82      0.74        76\n",
            "weighted avg       0.90      0.84      0.86        76\n",
            "\n",
            "torch.Size([76, 1])\n",
            "fold:  4\n",
            "\n",
            "training report: \n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.47      0.87      0.61        97\n",
            "         1.0       0.97      0.84      0.90       583\n",
            "\n",
            "    accuracy                           0.84       680\n",
            "   macro avg       0.72      0.85      0.76       680\n",
            "weighted avg       0.90      0.84      0.86       680\n",
            "\n",
            "testing report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.40      0.86      0.55         7\n",
            "         1.0       0.98      0.87      0.92        69\n",
            "\n",
            "    accuracy                           0.87        76\n",
            "   macro avg       0.69      0.86      0.73        76\n",
            "weighted avg       0.93      0.87      0.89        76\n",
            "\n",
            "torch.Size([76, 1])\n",
            "fold:  5\n",
            "\n",
            "training report: \n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.49      0.84      0.62       100\n",
            "         1.0       0.97      0.85      0.91       580\n",
            "\n",
            "    accuracy                           0.85       680\n",
            "   macro avg       0.73      0.84      0.76       680\n",
            "weighted avg       0.90      0.85      0.86       680\n",
            "\n",
            "testing report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.48      1.00      0.65        10\n",
            "         1.0       1.00      0.83      0.91        66\n",
            "\n",
            "    accuracy                           0.86        76\n",
            "   macro avg       0.74      0.92      0.78        76\n",
            "weighted avg       0.93      0.86      0.87        76\n",
            "\n",
            "torch.Size([76, 1])\n",
            "fold:  6\n",
            "\n",
            "training report: \n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.48      0.81      0.60        98\n",
            "         1.0       0.96      0.85      0.90       583\n",
            "\n",
            "    accuracy                           0.84       681\n",
            "   macro avg       0.72      0.83      0.75       681\n",
            "weighted avg       0.89      0.84      0.86       681\n",
            "\n",
            "testing report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.50      0.93      0.65        14\n",
            "         1.0       0.98      0.79      0.87        61\n",
            "\n",
            "    accuracy                           0.81        75\n",
            "   macro avg       0.74      0.86      0.76        75\n",
            "weighted avg       0.89      0.81      0.83        75\n",
            "\n",
            "torch.Size([75, 1])\n",
            "fold:  7\n",
            "\n",
            "training report: \n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.48      0.84      0.61       102\n",
            "         1.0       0.97      0.84      0.90       579\n",
            "\n",
            "    accuracy                           0.84       681\n",
            "   macro avg       0.73      0.84      0.76       681\n",
            "weighted avg       0.90      0.84      0.86       681\n",
            "\n",
            "testing report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.50      0.70      0.58        10\n",
            "         1.0       0.95      0.89      0.92        65\n",
            "\n",
            "    accuracy                           0.87        75\n",
            "   macro avg       0.73      0.80      0.75        75\n",
            "weighted avg       0.89      0.87      0.88        75\n",
            "\n",
            "torch.Size([75, 1])\n",
            "fold:  8\n",
            "\n",
            "training report: \n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.49      0.83      0.61       101\n",
            "         1.0       0.97      0.85      0.90       580\n",
            "\n",
            "    accuracy                           0.84       681\n",
            "   macro avg       0.73      0.84      0.76       681\n",
            "weighted avg       0.90      0.84      0.86       681\n",
            "\n",
            "testing report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.58      0.73      0.65        15\n",
            "         1.0       0.93      0.87      0.90        60\n",
            "\n",
            "    accuracy                           0.84        75\n",
            "   macro avg       0.75      0.80      0.77        75\n",
            "weighted avg       0.86      0.84      0.85        75\n",
            "\n",
            "torch.Size([75, 1])\n",
            "fold:  9\n",
            "\n",
            "training report: \n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.48      0.81      0.61        99\n",
            "         1.0       0.96      0.85      0.91       582\n",
            "\n",
            "    accuracy                           0.85       681\n",
            "   macro avg       0.72      0.83      0.76       681\n",
            "weighted avg       0.89      0.85      0.86       681\n",
            "\n",
            "testing report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.56      1.00      0.71        15\n",
            "         1.0       1.00      0.80      0.89        60\n",
            "\n",
            "    accuracy                           0.84        75\n",
            "   macro avg       0.78      0.90      0.80        75\n",
            "weighted avg       0.91      0.84      0.85        75\n",
            "\n",
            "torch.Size([75, 1])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "hEj4rX9xH6dC"
      },
      "source": [
        ""
      ],
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 298
        },
        "id": "yImmOUVaH6dC",
        "outputId": "4edcdfbc-1fe4-4c48-88e4-246ecea92d2d"
      },
      "source": [
        "for i in range(10):\n",
        "  plt.plot(losses[i])\n",
        "plt.title('losses')\n",
        "plt.ylim(0.25,.8)"
      ],
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.25, 0.8)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 82
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "Muup2X94H6dC"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}