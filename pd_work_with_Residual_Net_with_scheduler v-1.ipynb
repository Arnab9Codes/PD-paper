{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "language_info": {
      "name": "python",
      "version": "3.7.9",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "name": "  pd-work with Residual Net with scheduler.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Arnab9Codes/PD-paper/blob/main/pd_work_with_Residual_Net_with_scheduler%20v-1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vg72QPAqH-xY",
        "outputId": "44d4961e-488e-4139-c52e-4feb17086ab7"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eu8DgIgururO",
        "trusted": true
      },
      "source": [
        "## Initial Imports\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "import pandas as pd \n",
        "import numpy as np \n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import random\n",
        "from tqdm import tqdm_notebook as tqdm\n",
        "\n",
        "#import lime\n",
        "#import eli5\n",
        "#import mlxtend\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CwDybsyfM-2V"
      },
      "source": [
        "np.random.seed(10)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o4fLLdJVrurY",
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "10e2c450-92af-4a8e-adab-26e6ee4cd72b"
      },
      "source": [
        "\n",
        "pd_speech_features = pd.read_csv('/content/drive/My Drive/Paper work/Speech detection/pd_speech_features.csv')\n",
        "new_header = pd_speech_features.iloc[0] \n",
        "pd_speech_features = pd_speech_features[1:]\n",
        "pd_speech_features.columns = new_header \n",
        "pd_speech_features.head()\n",
        "print('Shape of the matrix is :', pd_speech_features.shape)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Shape of the matrix is : (756, 755)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7NIsR5gTrurh",
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 307
        },
        "outputId": "eacdadc9-cf93-4de8-bfb1-5f733b177d99"
      },
      "source": [
        "pd_speech_features.info() # Gives type of columns\n",
        "pd_speech_features.describe()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 756 entries, 1 to 756\n",
            "Columns: 755 entries, id to class\n",
            "dtypes: object(755)\n",
            "memory usage: 4.4+ MB\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>gender</th>\n",
              "      <th>PPE</th>\n",
              "      <th>DFA</th>\n",
              "      <th>RPDE</th>\n",
              "      <th>numPulses</th>\n",
              "      <th>numPeriodsPulses</th>\n",
              "      <th>meanPeriodPulses</th>\n",
              "      <th>stdDevPeriodPulses</th>\n",
              "      <th>locPctJitter</th>\n",
              "      <th>locAbsJitter</th>\n",
              "      <th>rapJitter</th>\n",
              "      <th>ppq5Jitter</th>\n",
              "      <th>ddpJitter</th>\n",
              "      <th>locShimmer</th>\n",
              "      <th>locDbShimmer</th>\n",
              "      <th>apq3Shimmer</th>\n",
              "      <th>apq5Shimmer</th>\n",
              "      <th>apq11Shimmer</th>\n",
              "      <th>ddaShimmer</th>\n",
              "      <th>meanAutoCorrHarmonicity</th>\n",
              "      <th>meanNoiseToHarmHarmonicity</th>\n",
              "      <th>meanHarmToNoiseHarmonicity</th>\n",
              "      <th>minIntensity</th>\n",
              "      <th>maxIntensity</th>\n",
              "      <th>meanIntensity</th>\n",
              "      <th>f1</th>\n",
              "      <th>f2</th>\n",
              "      <th>f3</th>\n",
              "      <th>f4</th>\n",
              "      <th>b1</th>\n",
              "      <th>b2</th>\n",
              "      <th>b3</th>\n",
              "      <th>b4</th>\n",
              "      <th>GQ_prc5_95</th>\n",
              "      <th>GQ_std_cycle_open</th>\n",
              "      <th>GQ_std_cycle_closed</th>\n",
              "      <th>GNE_mean</th>\n",
              "      <th>GNE_std</th>\n",
              "      <th>GNE_SNR_TKEO</th>\n",
              "      <th>...</th>\n",
              "      <th>tqwt_skewnessValue_dec_34</th>\n",
              "      <th>tqwt_skewnessValue_dec_35</th>\n",
              "      <th>tqwt_skewnessValue_dec_36</th>\n",
              "      <th>tqwt_kurtosisValue_dec_1</th>\n",
              "      <th>tqwt_kurtosisValue_dec_2</th>\n",
              "      <th>tqwt_kurtosisValue_dec_3</th>\n",
              "      <th>tqwt_kurtosisValue_dec_4</th>\n",
              "      <th>tqwt_kurtosisValue_dec_5</th>\n",
              "      <th>tqwt_kurtosisValue_dec_6</th>\n",
              "      <th>tqwt_kurtosisValue_dec_7</th>\n",
              "      <th>tqwt_kurtosisValue_dec_8</th>\n",
              "      <th>tqwt_kurtosisValue_dec_9</th>\n",
              "      <th>tqwt_kurtosisValue_dec_10</th>\n",
              "      <th>tqwt_kurtosisValue_dec_11</th>\n",
              "      <th>tqwt_kurtosisValue_dec_12</th>\n",
              "      <th>tqwt_kurtosisValue_dec_13</th>\n",
              "      <th>tqwt_kurtosisValue_dec_14</th>\n",
              "      <th>tqwt_kurtosisValue_dec_15</th>\n",
              "      <th>tqwt_kurtosisValue_dec_16</th>\n",
              "      <th>tqwt_kurtosisValue_dec_17</th>\n",
              "      <th>tqwt_kurtosisValue_dec_18</th>\n",
              "      <th>tqwt_kurtosisValue_dec_19</th>\n",
              "      <th>tqwt_kurtosisValue_dec_20</th>\n",
              "      <th>tqwt_kurtosisValue_dec_21</th>\n",
              "      <th>tqwt_kurtosisValue_dec_22</th>\n",
              "      <th>tqwt_kurtosisValue_dec_23</th>\n",
              "      <th>tqwt_kurtosisValue_dec_24</th>\n",
              "      <th>tqwt_kurtosisValue_dec_25</th>\n",
              "      <th>tqwt_kurtosisValue_dec_26</th>\n",
              "      <th>tqwt_kurtosisValue_dec_27</th>\n",
              "      <th>tqwt_kurtosisValue_dec_28</th>\n",
              "      <th>tqwt_kurtosisValue_dec_29</th>\n",
              "      <th>tqwt_kurtosisValue_dec_30</th>\n",
              "      <th>tqwt_kurtosisValue_dec_31</th>\n",
              "      <th>tqwt_kurtosisValue_dec_32</th>\n",
              "      <th>tqwt_kurtosisValue_dec_33</th>\n",
              "      <th>tqwt_kurtosisValue_dec_34</th>\n",
              "      <th>tqwt_kurtosisValue_dec_35</th>\n",
              "      <th>tqwt_kurtosisValue_dec_36</th>\n",
              "      <th>class</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>756</td>\n",
              "      <td>756</td>\n",
              "      <td>756</td>\n",
              "      <td>756</td>\n",
              "      <td>756</td>\n",
              "      <td>756</td>\n",
              "      <td>756</td>\n",
              "      <td>756</td>\n",
              "      <td>756</td>\n",
              "      <td>756</td>\n",
              "      <td>756</td>\n",
              "      <td>756</td>\n",
              "      <td>756</td>\n",
              "      <td>756</td>\n",
              "      <td>756</td>\n",
              "      <td>756</td>\n",
              "      <td>756</td>\n",
              "      <td>756</td>\n",
              "      <td>756</td>\n",
              "      <td>756</td>\n",
              "      <td>756</td>\n",
              "      <td>756</td>\n",
              "      <td>756</td>\n",
              "      <td>756</td>\n",
              "      <td>756</td>\n",
              "      <td>756</td>\n",
              "      <td>756</td>\n",
              "      <td>756</td>\n",
              "      <td>756</td>\n",
              "      <td>756</td>\n",
              "      <td>756</td>\n",
              "      <td>756</td>\n",
              "      <td>756</td>\n",
              "      <td>756</td>\n",
              "      <td>756</td>\n",
              "      <td>756</td>\n",
              "      <td>756</td>\n",
              "      <td>756</td>\n",
              "      <td>756</td>\n",
              "      <td>756</td>\n",
              "      <td>...</td>\n",
              "      <td>756</td>\n",
              "      <td>756</td>\n",
              "      <td>756</td>\n",
              "      <td>756</td>\n",
              "      <td>756</td>\n",
              "      <td>756</td>\n",
              "      <td>756</td>\n",
              "      <td>756</td>\n",
              "      <td>756</td>\n",
              "      <td>756</td>\n",
              "      <td>756</td>\n",
              "      <td>756</td>\n",
              "      <td>756</td>\n",
              "      <td>756</td>\n",
              "      <td>756</td>\n",
              "      <td>756</td>\n",
              "      <td>756</td>\n",
              "      <td>756</td>\n",
              "      <td>756</td>\n",
              "      <td>756</td>\n",
              "      <td>756</td>\n",
              "      <td>756</td>\n",
              "      <td>756</td>\n",
              "      <td>756</td>\n",
              "      <td>756</td>\n",
              "      <td>756</td>\n",
              "      <td>756</td>\n",
              "      <td>756</td>\n",
              "      <td>756</td>\n",
              "      <td>756</td>\n",
              "      <td>756</td>\n",
              "      <td>756</td>\n",
              "      <td>756</td>\n",
              "      <td>756</td>\n",
              "      <td>756</td>\n",
              "      <td>756</td>\n",
              "      <td>756</td>\n",
              "      <td>756</td>\n",
              "      <td>756</td>\n",
              "      <td>756</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>unique</th>\n",
              "      <td>252</td>\n",
              "      <td>2</td>\n",
              "      <td>740</td>\n",
              "      <td>745</td>\n",
              "      <td>748</td>\n",
              "      <td>315</td>\n",
              "      <td>319</td>\n",
              "      <td>755</td>\n",
              "      <td>646</td>\n",
              "      <td>358</td>\n",
              "      <td>543</td>\n",
              "      <td>183</td>\n",
              "      <td>244</td>\n",
              "      <td>327</td>\n",
              "      <td>734</td>\n",
              "      <td>547</td>\n",
              "      <td>710</td>\n",
              "      <td>709</td>\n",
              "      <td>722</td>\n",
              "      <td>741</td>\n",
              "      <td>747</td>\n",
              "      <td>748</td>\n",
              "      <td>745</td>\n",
              "      <td>755</td>\n",
              "      <td>755</td>\n",
              "      <td>755</td>\n",
              "      <td>755</td>\n",
              "      <td>755</td>\n",
              "      <td>755</td>\n",
              "      <td>755</td>\n",
              "      <td>755</td>\n",
              "      <td>755</td>\n",
              "      <td>755</td>\n",
              "      <td>754</td>\n",
              "      <td>356</td>\n",
              "      <td>753</td>\n",
              "      <td>380</td>\n",
              "      <td>743</td>\n",
              "      <td>750</td>\n",
              "      <td>735</td>\n",
              "      <td>...</td>\n",
              "      <td>753</td>\n",
              "      <td>754</td>\n",
              "      <td>754</td>\n",
              "      <td>755</td>\n",
              "      <td>754</td>\n",
              "      <td>754</td>\n",
              "      <td>752</td>\n",
              "      <td>754</td>\n",
              "      <td>754</td>\n",
              "      <td>751</td>\n",
              "      <td>752</td>\n",
              "      <td>753</td>\n",
              "      <td>752</td>\n",
              "      <td>754</td>\n",
              "      <td>751</td>\n",
              "      <td>749</td>\n",
              "      <td>741</td>\n",
              "      <td>744</td>\n",
              "      <td>742</td>\n",
              "      <td>742</td>\n",
              "      <td>740</td>\n",
              "      <td>738</td>\n",
              "      <td>746</td>\n",
              "      <td>735</td>\n",
              "      <td>739</td>\n",
              "      <td>732</td>\n",
              "      <td>728</td>\n",
              "      <td>723</td>\n",
              "      <td>735</td>\n",
              "      <td>750</td>\n",
              "      <td>749</td>\n",
              "      <td>755</td>\n",
              "      <td>752</td>\n",
              "      <td>753</td>\n",
              "      <td>749</td>\n",
              "      <td>752</td>\n",
              "      <td>753</td>\n",
              "      <td>753</td>\n",
              "      <td>754</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>top</th>\n",
              "      <td>13</td>\n",
              "      <td>1</td>\n",
              "      <td>0.82273</td>\n",
              "      <td>0.65177</td>\n",
              "      <td>0.70689</td>\n",
              "      <td>237</td>\n",
              "      <td>332</td>\n",
              "      <td>0.006004477</td>\n",
              "      <td>5.69E-05</td>\n",
              "      <td>0.00076</td>\n",
              "      <td>1.39E-05</td>\n",
              "      <td>0.00012</td>\n",
              "      <td>0.00032</td>\n",
              "      <td>0.00036</td>\n",
              "      <td>0.05227</td>\n",
              "      <td>0.376</td>\n",
              "      <td>0.04839</td>\n",
              "      <td>0.01948</td>\n",
              "      <td>0.02824</td>\n",
              "      <td>0.14516</td>\n",
              "      <td>0.998392</td>\n",
              "      <td>0.001613</td>\n",
              "      <td>18.344</td>\n",
              "      <td>80.44328048</td>\n",
              "      <td>85.61022901</td>\n",
              "      <td>82.73382546</td>\n",
              "      <td>769.6772481</td>\n",
              "      <td>1279.806544</td>\n",
              "      <td>3107.527459</td>\n",
              "      <td>3738.070756</td>\n",
              "      <td>76.4915472</td>\n",
              "      <td>117.5349021</td>\n",
              "      <td>192.2014234</td>\n",
              "      <td>299.1940012</td>\n",
              "      <td>1</td>\n",
              "      <td>6.9051</td>\n",
              "      <td>0</td>\n",
              "      <td>1.0221</td>\n",
              "      <td>0.10661</td>\n",
              "      <td>0.1442</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.091461</td>\n",
              "      <td>-2.7179</td>\n",
              "      <td>-8.9141</td>\n",
              "      <td>44.7366</td>\n",
              "      <td>15.7456</td>\n",
              "      <td>6.2387</td>\n",
              "      <td>9.1907</td>\n",
              "      <td>11.6205</td>\n",
              "      <td>4.1187</td>\n",
              "      <td>4.7652</td>\n",
              "      <td>3.7508</td>\n",
              "      <td>4.3825</td>\n",
              "      <td>4.5152</td>\n",
              "      <td>4.3659</td>\n",
              "      <td>3.7173</td>\n",
              "      <td>2.6395</td>\n",
              "      <td>2.4171</td>\n",
              "      <td>2.9004</td>\n",
              "      <td>2.2731</td>\n",
              "      <td>2.5803</td>\n",
              "      <td>2.1907</td>\n",
              "      <td>2.4863</td>\n",
              "      <td>2.9735</td>\n",
              "      <td>1.5932</td>\n",
              "      <td>1.6444</td>\n",
              "      <td>1.7484</td>\n",
              "      <td>2.3105</td>\n",
              "      <td>1.5842</td>\n",
              "      <td>1.6337</td>\n",
              "      <td>1.613</td>\n",
              "      <td>3.4835</td>\n",
              "      <td>4.0251</td>\n",
              "      <td>4.4021</td>\n",
              "      <td>4.2105</td>\n",
              "      <td>2.3974</td>\n",
              "      <td>3.1144</td>\n",
              "      <td>6.2712</td>\n",
              "      <td>4.2391</td>\n",
              "      <td>10.0693</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>freq</th>\n",
              "      <td>3</td>\n",
              "      <td>390</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>9</td>\n",
              "      <td>8</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>9</td>\n",
              "      <td>10</td>\n",
              "      <td>24</td>\n",
              "      <td>14</td>\n",
              "      <td>11</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>385</td>\n",
              "      <td>2</td>\n",
              "      <td>377</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>...</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>564</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>4 rows Ã— 755 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "0        id gender  ... tqwt_kurtosisValue_dec_36 class\n",
              "count   756    756  ...                       756   756\n",
              "unique  252      2  ...                       754     2\n",
              "top      13      1  ...                   10.0693     1\n",
              "freq      3    390  ...                         2   564\n",
              "\n",
              "[4 rows x 755 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a9FvWGzGrurv",
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 142
        },
        "outputId": "4f1d9614-9595-4a0b-aea0-afbbd62ddee3"
      },
      "source": [
        "pd_speech_features['patient/healthy count'] = 1\n",
        "pd_speech_features.groupby('class').sum()/3"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>patient/healthy count</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>class</th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>64.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>188.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "0      patient/healthy count\n",
              "class                       \n",
              "0                       64.0\n",
              "1                      188.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L8ppw8fnrur1",
        "trusted": true
      },
      "source": [
        "pd_speech_features = pd_speech_features.drop(['patient/healthy count'], axis = 1)  #756x755"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BY5oJXz9rur4",
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b40eb586-5168-4f91-c6ef-003651198ac3"
      },
      "source": [
        "pd_speech_features =  pd_speech_features.astype(float) #per default all floats \n",
        "pd_speech_features[['id', 'numPulses', 'numPeriodsPulses']] = pd_speech_features[['id', 'numPulses', 'numPeriodsPulses']].astype(int) #ints\n",
        "pd_speech_features[['gender', 'class']] = pd_speech_features[['gender', 'class']].astype('category') #categoricals\n",
        "pd_speech_features.dtypes"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0\n",
              "id                              int64\n",
              "gender                       category\n",
              "PPE                           float64\n",
              "DFA                           float64\n",
              "RPDE                          float64\n",
              "                               ...   \n",
              "tqwt_kurtosisValue_dec_33     float64\n",
              "tqwt_kurtosisValue_dec_34     float64\n",
              "tqwt_kurtosisValue_dec_35     float64\n",
              "tqwt_kurtosisValue_dec_36     float64\n",
              "class                        category\n",
              "Length: 755, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WBwpHHK3rur9",
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 253
        },
        "outputId": "e2d3237c-b0ea-4275-9bdd-7d6a1220c61e"
      },
      "source": [
        "pd_speech_features_no_tqwt = pd_speech_features[pd_speech_features.columns[1: -433]]\n",
        "pd_speech_features_no_tqwt.head()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>gender</th>\n",
              "      <th>PPE</th>\n",
              "      <th>DFA</th>\n",
              "      <th>RPDE</th>\n",
              "      <th>numPulses</th>\n",
              "      <th>numPeriodsPulses</th>\n",
              "      <th>meanPeriodPulses</th>\n",
              "      <th>stdDevPeriodPulses</th>\n",
              "      <th>locPctJitter</th>\n",
              "      <th>locAbsJitter</th>\n",
              "      <th>rapJitter</th>\n",
              "      <th>ppq5Jitter</th>\n",
              "      <th>ddpJitter</th>\n",
              "      <th>locShimmer</th>\n",
              "      <th>locDbShimmer</th>\n",
              "      <th>apq3Shimmer</th>\n",
              "      <th>apq5Shimmer</th>\n",
              "      <th>apq11Shimmer</th>\n",
              "      <th>ddaShimmer</th>\n",
              "      <th>meanAutoCorrHarmonicity</th>\n",
              "      <th>meanNoiseToHarmHarmonicity</th>\n",
              "      <th>meanHarmToNoiseHarmonicity</th>\n",
              "      <th>minIntensity</th>\n",
              "      <th>maxIntensity</th>\n",
              "      <th>meanIntensity</th>\n",
              "      <th>f1</th>\n",
              "      <th>f2</th>\n",
              "      <th>f3</th>\n",
              "      <th>f4</th>\n",
              "      <th>b1</th>\n",
              "      <th>b2</th>\n",
              "      <th>b3</th>\n",
              "      <th>b4</th>\n",
              "      <th>GQ_prc5_95</th>\n",
              "      <th>GQ_std_cycle_open</th>\n",
              "      <th>GQ_std_cycle_closed</th>\n",
              "      <th>GNE_mean</th>\n",
              "      <th>GNE_std</th>\n",
              "      <th>GNE_SNR_TKEO</th>\n",
              "      <th>GNE_SNR_SEO</th>\n",
              "      <th>...</th>\n",
              "      <th>app_LT_entropy_shannon_1_coef</th>\n",
              "      <th>app_LT_entropy_shannon_2_coef</th>\n",
              "      <th>app_LT_entropy_shannon_3_coef</th>\n",
              "      <th>app_LT_entropy_shannon_4_coef</th>\n",
              "      <th>app_LT_entropy_shannon_5_coef</th>\n",
              "      <th>app_LT_entropy_shannon_6_coef</th>\n",
              "      <th>app_LT_entropy_shannon_7_coef</th>\n",
              "      <th>app_LT_entropy_shannon_8_coef</th>\n",
              "      <th>app_LT_entropy_shannon_9_coef</th>\n",
              "      <th>app_LT_entropy_shannon_10_coef</th>\n",
              "      <th>app_LT_entropy_log_1_coef</th>\n",
              "      <th>app_LT_entropy_log_2_coef</th>\n",
              "      <th>app_LT_entropy_log_3_coef</th>\n",
              "      <th>app_LT_entropy_log_4_coef</th>\n",
              "      <th>app_LT_entropy_log_5_coef</th>\n",
              "      <th>app_LT_entropy_log_6_coef</th>\n",
              "      <th>app_LT_entropy_log_7_coef</th>\n",
              "      <th>app_LT_entropy_log_8_coef</th>\n",
              "      <th>app_LT_entropy_log_9_coef</th>\n",
              "      <th>app_LT_entropy_log_10_coef</th>\n",
              "      <th>app_LT_TKEO_mean_1_coef</th>\n",
              "      <th>app_LT_TKEO_mean_2_coef</th>\n",
              "      <th>app_LT_TKEO_mean_3_coef</th>\n",
              "      <th>app_LT_TKEO_mean_4_coef</th>\n",
              "      <th>app_LT_TKEO_mean_5_coef</th>\n",
              "      <th>app_LT_TKEO_mean_6_coef</th>\n",
              "      <th>app_LT_TKEO_mean_7_coef</th>\n",
              "      <th>app_LT_TKEO_mean_8_coef</th>\n",
              "      <th>app_LT_TKEO_mean_9_coef</th>\n",
              "      <th>app_LT_TKEO_mean_10_coef</th>\n",
              "      <th>app_LT_TKEO_std_1_coef</th>\n",
              "      <th>app_LT_TKEO_std_2_coef</th>\n",
              "      <th>app_LT_TKEO_std_3_coef</th>\n",
              "      <th>app_LT_TKEO_std_4_coef</th>\n",
              "      <th>app_LT_TKEO_std_5_coef</th>\n",
              "      <th>app_LT_TKEO_std_6_coef</th>\n",
              "      <th>app_LT_TKEO_std_7_coef</th>\n",
              "      <th>app_LT_TKEO_std_8_coef</th>\n",
              "      <th>app_LT_TKEO_std_9_coef</th>\n",
              "      <th>app_LT_TKEO_std_10_coef</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1.0</td>\n",
              "      <td>0.85247</td>\n",
              "      <td>0.71826</td>\n",
              "      <td>0.57227</td>\n",
              "      <td>240</td>\n",
              "      <td>239</td>\n",
              "      <td>0.008064</td>\n",
              "      <td>0.000087</td>\n",
              "      <td>0.00218</td>\n",
              "      <td>0.000018</td>\n",
              "      <td>0.00067</td>\n",
              "      <td>0.00129</td>\n",
              "      <td>0.00200</td>\n",
              "      <td>0.05883</td>\n",
              "      <td>0.517</td>\n",
              "      <td>0.03011</td>\n",
              "      <td>0.03496</td>\n",
              "      <td>0.04828</td>\n",
              "      <td>0.09034</td>\n",
              "      <td>0.970805</td>\n",
              "      <td>0.036223</td>\n",
              "      <td>18.995</td>\n",
              "      <td>69.997496</td>\n",
              "      <td>76.088046</td>\n",
              "      <td>72.465512</td>\n",
              "      <td>539.342735</td>\n",
              "      <td>1031.849040</td>\n",
              "      <td>2447.162183</td>\n",
              "      <td>3655.054806</td>\n",
              "      <td>101.092218</td>\n",
              "      <td>83.147440</td>\n",
              "      <td>255.214830</td>\n",
              "      <td>396.643631</td>\n",
              "      <td>0.77778</td>\n",
              "      <td>11.7245</td>\n",
              "      <td>2.8277</td>\n",
              "      <td>1.17300</td>\n",
              "      <td>0.26512</td>\n",
              "      <td>0.083127</td>\n",
              "      <td>1200445.612</td>\n",
              "      <td>...</td>\n",
              "      <td>-19278.0371</td>\n",
              "      <td>-25711.8622</td>\n",
              "      <td>-36938.1370</td>\n",
              "      <td>-57264.6625</td>\n",
              "      <td>-98433.1856</td>\n",
              "      <td>-184901.7535</td>\n",
              "      <td>-381059.3510</td>\n",
              "      <td>-776445.2329</td>\n",
              "      <td>-1676725.978</td>\n",
              "      <td>-3601122.613</td>\n",
              "      <td>414.6434</td>\n",
              "      <td>276.4850</td>\n",
              "      <td>198.5803</td>\n",
              "      <td>153.8978</td>\n",
              "      <td>132.2489</td>\n",
              "      <td>124.1971</td>\n",
              "      <td>127.9812</td>\n",
              "      <td>130.3804</td>\n",
              "      <td>140.7776</td>\n",
              "      <td>151.1748</td>\n",
              "      <td>0.86121</td>\n",
              "      <td>3.0487</td>\n",
              "      <td>9.7825</td>\n",
              "      <td>28.5949</td>\n",
              "      <td>74.3411</td>\n",
              "      <td>174.9214</td>\n",
              "      <td>371.7296</td>\n",
              "      <td>793.0680</td>\n",
              "      <td>1586.1824</td>\n",
              "      <td>3173.0448</td>\n",
              "      <td>6.2990</td>\n",
              "      <td>16.7003</td>\n",
              "      <td>42.0762</td>\n",
              "      <td>101.0889</td>\n",
              "      <td>228.8489</td>\n",
              "      <td>493.8563</td>\n",
              "      <td>1015.7707</td>\n",
              "      <td>2091.9460</td>\n",
              "      <td>4188.2456</td>\n",
              "      <td>8373.9278</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1.0</td>\n",
              "      <td>0.76686</td>\n",
              "      <td>0.69481</td>\n",
              "      <td>0.53966</td>\n",
              "      <td>234</td>\n",
              "      <td>233</td>\n",
              "      <td>0.008258</td>\n",
              "      <td>0.000073</td>\n",
              "      <td>0.00195</td>\n",
              "      <td>0.000016</td>\n",
              "      <td>0.00052</td>\n",
              "      <td>0.00112</td>\n",
              "      <td>0.00157</td>\n",
              "      <td>0.05516</td>\n",
              "      <td>0.502</td>\n",
              "      <td>0.02320</td>\n",
              "      <td>0.03675</td>\n",
              "      <td>0.06195</td>\n",
              "      <td>0.06961</td>\n",
              "      <td>0.984322</td>\n",
              "      <td>0.017974</td>\n",
              "      <td>21.497</td>\n",
              "      <td>67.415903</td>\n",
              "      <td>73.046374</td>\n",
              "      <td>71.528945</td>\n",
              "      <td>564.363615</td>\n",
              "      <td>1016.367294</td>\n",
              "      <td>2383.565201</td>\n",
              "      <td>3498.681572</td>\n",
              "      <td>58.465428</td>\n",
              "      <td>86.487292</td>\n",
              "      <td>248.357127</td>\n",
              "      <td>218.229722</td>\n",
              "      <td>0.81250</td>\n",
              "      <td>13.8284</td>\n",
              "      <td>2.8908</td>\n",
              "      <td>1.02210</td>\n",
              "      <td>0.22004</td>\n",
              "      <td>0.127410</td>\n",
              "      <td>1298455.445</td>\n",
              "      <td>...</td>\n",
              "      <td>-19028.6532</td>\n",
              "      <td>-25392.0069</td>\n",
              "      <td>-36496.8101</td>\n",
              "      <td>-56599.2563</td>\n",
              "      <td>-97324.8830</td>\n",
              "      <td>-182880.5032</td>\n",
              "      <td>-376979.9939</td>\n",
              "      <td>-768230.2335</td>\n",
              "      <td>-1659120.382</td>\n",
              "      <td>-3563560.603</td>\n",
              "      <td>413.5284</td>\n",
              "      <td>275.8597</td>\n",
              "      <td>198.1971</td>\n",
              "      <td>153.6379</td>\n",
              "      <td>132.0522</td>\n",
              "      <td>124.0327</td>\n",
              "      <td>127.8282</td>\n",
              "      <td>130.2373</td>\n",
              "      <td>140.6345</td>\n",
              "      <td>151.0317</td>\n",
              "      <td>0.85289</td>\n",
              "      <td>3.0213</td>\n",
              "      <td>9.6956</td>\n",
              "      <td>28.3506</td>\n",
              "      <td>73.7185</td>\n",
              "      <td>173.4666</td>\n",
              "      <td>368.5705</td>\n",
              "      <td>786.0763</td>\n",
              "      <td>1572.1837</td>\n",
              "      <td>3144.4525</td>\n",
              "      <td>6.2381</td>\n",
              "      <td>16.5376</td>\n",
              "      <td>41.7306</td>\n",
              "      <td>100.0918</td>\n",
              "      <td>226.9019</td>\n",
              "      <td>489.9169</td>\n",
              "      <td>1006.3702</td>\n",
              "      <td>2074.4541</td>\n",
              "      <td>4148.9889</td>\n",
              "      <td>8298.1606</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1.0</td>\n",
              "      <td>0.85083</td>\n",
              "      <td>0.67604</td>\n",
              "      <td>0.58982</td>\n",
              "      <td>232</td>\n",
              "      <td>231</td>\n",
              "      <td>0.008340</td>\n",
              "      <td>0.000060</td>\n",
              "      <td>0.00176</td>\n",
              "      <td>0.000015</td>\n",
              "      <td>0.00057</td>\n",
              "      <td>0.00111</td>\n",
              "      <td>0.00171</td>\n",
              "      <td>0.09902</td>\n",
              "      <td>0.897</td>\n",
              "      <td>0.05094</td>\n",
              "      <td>0.06497</td>\n",
              "      <td>0.07772</td>\n",
              "      <td>0.15282</td>\n",
              "      <td>0.974846</td>\n",
              "      <td>0.026313</td>\n",
              "      <td>17.651</td>\n",
              "      <td>62.661706</td>\n",
              "      <td>71.633549</td>\n",
              "      <td>68.086583</td>\n",
              "      <td>548.444604</td>\n",
              "      <td>1032.406341</td>\n",
              "      <td>2357.826954</td>\n",
              "      <td>3678.128717</td>\n",
              "      <td>160.387771</td>\n",
              "      <td>54.685168</td>\n",
              "      <td>151.694847</td>\n",
              "      <td>84.240339</td>\n",
              "      <td>0.81818</td>\n",
              "      <td>26.9273</td>\n",
              "      <td>2.6975</td>\n",
              "      <td>0.84951</td>\n",
              "      <td>0.15756</td>\n",
              "      <td>0.116890</td>\n",
              "      <td>1272869.841</td>\n",
              "      <td>...</td>\n",
              "      <td>-18926.4578</td>\n",
              "      <td>-25253.6144</td>\n",
              "      <td>-36288.7542</td>\n",
              "      <td>-56258.4752</td>\n",
              "      <td>-96708.1119</td>\n",
              "      <td>-181663.4768</td>\n",
              "      <td>-374463.8517</td>\n",
              "      <td>-763056.6385</td>\n",
              "      <td>-1648032.246</td>\n",
              "      <td>-3539902.400</td>\n",
              "      <td>413.0675</td>\n",
              "      <td>275.5868</td>\n",
              "      <td>198.0149</td>\n",
              "      <td>153.5036</td>\n",
              "      <td>131.9418</td>\n",
              "      <td>123.9329</td>\n",
              "      <td>127.7329</td>\n",
              "      <td>130.1464</td>\n",
              "      <td>140.5436</td>\n",
              "      <td>150.9408</td>\n",
              "      <td>0.84977</td>\n",
              "      <td>3.0062</td>\n",
              "      <td>9.6470</td>\n",
              "      <td>28.2014</td>\n",
              "      <td>73.3297</td>\n",
              "      <td>172.5204</td>\n",
              "      <td>366.5936</td>\n",
              "      <td>782.0604</td>\n",
              "      <td>1564.1081</td>\n",
              "      <td>3128.0295</td>\n",
              "      <td>6.2163</td>\n",
              "      <td>16.4817</td>\n",
              "      <td>41.4869</td>\n",
              "      <td>99.6154</td>\n",
              "      <td>225.7803</td>\n",
              "      <td>486.9865</td>\n",
              "      <td>1001.7348</td>\n",
              "      <td>2064.1067</td>\n",
              "      <td>4127.0967</td>\n",
              "      <td>8254.7868</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.41121</td>\n",
              "      <td>0.79672</td>\n",
              "      <td>0.59257</td>\n",
              "      <td>178</td>\n",
              "      <td>177</td>\n",
              "      <td>0.010858</td>\n",
              "      <td>0.000183</td>\n",
              "      <td>0.00419</td>\n",
              "      <td>0.000046</td>\n",
              "      <td>0.00149</td>\n",
              "      <td>0.00268</td>\n",
              "      <td>0.00446</td>\n",
              "      <td>0.05451</td>\n",
              "      <td>0.527</td>\n",
              "      <td>0.02395</td>\n",
              "      <td>0.02857</td>\n",
              "      <td>0.04462</td>\n",
              "      <td>0.07185</td>\n",
              "      <td>0.968343</td>\n",
              "      <td>0.042003</td>\n",
              "      <td>19.865</td>\n",
              "      <td>76.306989</td>\n",
              "      <td>81.000749</td>\n",
              "      <td>79.190593</td>\n",
              "      <td>819.529588</td>\n",
              "      <td>1201.813897</td>\n",
              "      <td>3154.035654</td>\n",
              "      <td>4122.163933</td>\n",
              "      <td>238.667052</td>\n",
              "      <td>191.984916</td>\n",
              "      <td>573.752909</td>\n",
              "      <td>526.147599</td>\n",
              "      <td>0.98548</td>\n",
              "      <td>139.5744</td>\n",
              "      <td>1.6961</td>\n",
              "      <td>0.83405</td>\n",
              "      <td>0.17295</td>\n",
              "      <td>0.147370</td>\n",
              "      <td>1932289.206</td>\n",
              "      <td>...</td>\n",
              "      <td>-19352.0891</td>\n",
              "      <td>-25452.5218</td>\n",
              "      <td>-35824.8476</td>\n",
              "      <td>-54370.9290</td>\n",
              "      <td>-91686.1704</td>\n",
              "      <td>-169639.1274</td>\n",
              "      <td>-347128.0292</td>\n",
              "      <td>-704471.7514</td>\n",
              "      <td>-1522358.498</td>\n",
              "      <td>-3271399.011</td>\n",
              "      <td>413.6380</td>\n",
              "      <td>275.3259</td>\n",
              "      <td>197.2795</td>\n",
              "      <td>152.5940</td>\n",
              "      <td>130.9480</td>\n",
              "      <td>122.8786</td>\n",
              "      <td>126.6411</td>\n",
              "      <td>129.0689</td>\n",
              "      <td>139.4666</td>\n",
              "      <td>149.8649</td>\n",
              "      <td>0.88367</td>\n",
              "      <td>2.9398</td>\n",
              "      <td>9.0446</td>\n",
              "      <td>26.2220</td>\n",
              "      <td>67.4885</td>\n",
              "      <td>158.1634</td>\n",
              "      <td>336.9109</td>\n",
              "      <td>724.9443</td>\n",
              "      <td>1448.7625</td>\n",
              "      <td>2913.3877</td>\n",
              "      <td>6.7833</td>\n",
              "      <td>16.8216</td>\n",
              "      <td>41.3157</td>\n",
              "      <td>94.4579</td>\n",
              "      <td>211.1565</td>\n",
              "      <td>443.3447</td>\n",
              "      <td>955.8128</td>\n",
              "      <td>1890.1299</td>\n",
              "      <td>3910.7029</td>\n",
              "      <td>7698.9389</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.32790</td>\n",
              "      <td>0.79782</td>\n",
              "      <td>0.53028</td>\n",
              "      <td>236</td>\n",
              "      <td>235</td>\n",
              "      <td>0.008162</td>\n",
              "      <td>0.002669</td>\n",
              "      <td>0.00535</td>\n",
              "      <td>0.000044</td>\n",
              "      <td>0.00166</td>\n",
              "      <td>0.00227</td>\n",
              "      <td>0.00499</td>\n",
              "      <td>0.05610</td>\n",
              "      <td>0.497</td>\n",
              "      <td>0.02909</td>\n",
              "      <td>0.03327</td>\n",
              "      <td>0.05278</td>\n",
              "      <td>0.08728</td>\n",
              "      <td>0.975754</td>\n",
              "      <td>0.027139</td>\n",
              "      <td>19.557</td>\n",
              "      <td>76.645686</td>\n",
              "      <td>80.937258</td>\n",
              "      <td>79.183495</td>\n",
              "      <td>846.796144</td>\n",
              "      <td>1215.346469</td>\n",
              "      <td>3201.513132</td>\n",
              "      <td>4085.456839</td>\n",
              "      <td>402.216738</td>\n",
              "      <td>210.061394</td>\n",
              "      <td>203.637106</td>\n",
              "      <td>384.611697</td>\n",
              "      <td>0.97847</td>\n",
              "      <td>102.0549</td>\n",
              "      <td>15.4045</td>\n",
              "      <td>0.83556</td>\n",
              "      <td>0.16210</td>\n",
              "      <td>0.151990</td>\n",
              "      <td>1861807.802</td>\n",
              "      <td>...</td>\n",
              "      <td>-21066.0878</td>\n",
              "      <td>-27472.1915</td>\n",
              "      <td>-38399.3353</td>\n",
              "      <td>-57514.4436</td>\n",
              "      <td>-95740.4317</td>\n",
              "      <td>-176007.1589</td>\n",
              "      <td>-359900.4024</td>\n",
              "      <td>-725678.1466</td>\n",
              "      <td>-1567705.131</td>\n",
              "      <td>-3367165.736</td>\n",
              "      <td>421.1396</td>\n",
              "      <td>279.1703</td>\n",
              "      <td>199.4350</td>\n",
              "      <td>153.7788</td>\n",
              "      <td>131.6258</td>\n",
              "      <td>123.3566</td>\n",
              "      <td>127.0837</td>\n",
              "      <td>129.4029</td>\n",
              "      <td>139.8011</td>\n",
              "      <td>150.2018</td>\n",
              "      <td>0.94619</td>\n",
              "      <td>3.2838</td>\n",
              "      <td>10.5898</td>\n",
              "      <td>30.3224</td>\n",
              "      <td>78.2530</td>\n",
              "      <td>182.9658</td>\n",
              "      <td>390.1740</td>\n",
              "      <td>827.9359</td>\n",
              "      <td>1653.5676</td>\n",
              "      <td>3265.5292</td>\n",
              "      <td>6.9366</td>\n",
              "      <td>18.3595</td>\n",
              "      <td>46.2704</td>\n",
              "      <td>108.6792</td>\n",
              "      <td>244.0607</td>\n",
              "      <td>541.2414</td>\n",
              "      <td>1057.2566</td>\n",
              "      <td>2242.5460</td>\n",
              "      <td>4297.4639</td>\n",
              "      <td>8645.2845</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows Ã— 321 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "0 gender      PPE  ...  app_LT_TKEO_std_9_coef  app_LT_TKEO_std_10_coef\n",
              "1    1.0  0.85247  ...               4188.2456                8373.9278\n",
              "2    1.0  0.76686  ...               4148.9889                8298.1606\n",
              "3    1.0  0.85083  ...               4127.0967                8254.7868\n",
              "4    0.0  0.41121  ...               3910.7029                7698.9389\n",
              "5    0.0  0.32790  ...               4297.4639                8645.2845\n",
              "\n",
              "[5 rows x 321 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uQ8SbaiTCRqP",
        "trusted": true
      },
      "source": [
        "train_df=pd_speech_features\n",
        "train_df_tqwt=pd_speech_features_no_tqwt\n",
        "\n",
        "y_train = train_df['class']\n",
        "y_train = np.array(y_train.values, dtype = 'int')\n",
        "x_train = train_df.drop(['class','id'], axis = 1) \n",
        "\n",
        "x_train = x_train.values\n",
        "#y_validation = validation_df['class']\n",
        "#y_validation = np.array(y_validation.values, dtype = 'int')\n",
        "#x_validation = validation_df.drop(['class'], axis = 1) "
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gOPAWkkDH6c6",
        "outputId": "f056b0be-0603-42bb-f300-5ecc48170b9d"
      },
      "source": [
        "features=train_df.columns.values.tolist()\n",
        "features.remove('id')\n",
        "features.remove('class')\n",
        "print(len(features))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "753\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QkotwMAfqW6W",
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "63d5b6ce-9a01-488f-83b1-3e7b08c880c4"
      },
      "source": [
        "print(x_train.shape)\n",
        "print(y_train.shape)\n"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(756, 753)\n",
            "(756,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tteEg1QRItRm",
        "outputId": "01e5e54a-7599-4861-dd34-2cebe07d1ff8"
      },
      "source": [
        "!pip install --upgrade scikit-learn"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already up-to-date: scikit-learn in /usr/local/lib/python3.7/dist-packages (0.24.1)\n",
            "Requirement already satisfied, skipping upgrade: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (2.1.0)\n",
            "Requirement already satisfied, skipping upgrade: scipy>=0.19.1 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (1.4.1)\n",
            "Requirement already satisfied, skipping upgrade: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (1.0.1)\n",
            "Requirement already satisfied, skipping upgrade: numpy>=1.13.3 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (1.19.5)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xZgiEXM-XTxv",
        "trusted": true
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "sc = StandardScaler()\n",
        "\n",
        "x_train= sc.fit_transform(x_train)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "5v1oVAU-H6c7"
      },
      "source": [
        "from sklearn.feature_selection import SequentialFeatureSelector\n",
        "sfs_9 = SequentialFeatureSelector(LogisticRegression(random_state=10),n_features_to_select=9, n_jobs=-1)\n",
        "\n",
        "sfs_9=sfs_9.fit(x_train, y_train)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gWZL5YClH6c9",
        "outputId": "7e05980f-7388-458d-bf9f-1c9c814c2b3d"
      },
      "source": [
        "x_train_transformed_9_features=sfs_9.transform(x_train)\n",
        "x_train_transformed_9_features.shape"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(756, 9)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bIgtx3FbTc4t"
      },
      "source": [
        "# LSTM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lPtdGmwVmeEI",
        "trusted": true
      },
      "source": [
        "from sklearn.model_selection import LeaveOneOut, KFold\n",
        "from sklearn.metrics import accuracy_score, f1_score, classification_report"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1McWKloGrusQ",
        "trusted": true
      },
      "source": [
        "from sklearn import svm \n",
        "from sklearn.model_selection import cross_val_score, GridSearchCV\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "\n",
        "param_grid = {'C': [0.1, 1, 10, 100, 1000],  \n",
        "              'gamma': [1, 0.1, 0.01, 0.001, 0.0001], \n",
        "              'kernel': ['rbf']}  "
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VOlnHs_wprt5",
        "trusted": true
      },
      "source": [
        "k_fold=KFold(10,True,10)"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "eXuxV9-3H6c_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "650568bd-9482-4969-a304-e7073445728e"
      },
      "source": [
        "import torch.nn as nn\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "torch.manual_seed(10)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7f1e12fdcaf0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "Glj55X8yH6c_"
      },
      "source": [
        "m=nn.Sequential(nn.Linear(18,18), nn.ReLU(),nn.Linear(18,18),nn.ReLU())\n",
        "\n",
        "class Model(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.l1=nn.Linear(9,18)\n",
        "        self.l2=nn.Linear(18,18)\n",
        "        self.l3=nn.Linear(18,18)\n",
        "        self.ll=m\n",
        "        self.l4=nn.Linear(18,1)\n",
        "        \n",
        "    def forward(self,x):\n",
        "        x=self.l1(x)\n",
        "        x=F.relu(F.dropout(x,0.200))\n",
        "        x1=self.l2(x)\n",
        "        x2=F.relu(F.dropout(x,0.20))\n",
        "        x=self.l3(x)\n",
        "        x=self.ll((x+x1)/2.00)\n",
        "        x=F.relu(F.dropout(x,.200))\n",
        "        x3=self.ll((x))\n",
        "        x=self.l3(x)\n",
        "        x=F.relu(F.dropout((x+x1)/2.0,0.000))\n",
        "        x=self.l4(x)\n",
        "        \n",
        "        return x\n",
        "        \n"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "aGkB3k10H6c_"
      },
      "source": [
        "#x=torch.from_numpy(x_train_transformed_9_features[:600]).float()\n",
        "#x=torch.from_numpy(x_train[:600]).float()\n",
        "#y=torch.from_numpy(y_train[:600]).float()\n",
        "\n",
        "def train(model,optimizer, scheduler, criterion, x,y):\n",
        "    \n",
        "    losses=[]\n",
        "    \n",
        "    for i in range(100):\n",
        "        output=model(x)\n",
        "        loss=criterion(output.view(y.shape[0]),y)\n",
        "    \n",
        "        losses.append(loss)\n",
        "    \n",
        "        #if (i%100==0):\n",
        "         #   print(i,' ',loss.item())\n",
        "    \n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "    \n",
        "    # training accuracy\n",
        "    with torch.no_grad():\n",
        "        preds=torch.sigmoid(model(x)).round()\n",
        "        print('\\ntraining report: \\n',classification_report(preds.view(preds.shape[0]),y))\n",
        "        \n",
        "    return model, losses\n",
        "\n",
        "#model, losses= train(model,optimizer, scheduler, criterion,x,y)"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "5Za9Fg9eH6dA"
      },
      "source": [
        "def test(model, tstx, tsty):\n",
        "    with torch.no_grad():\n",
        "    \n",
        "        preds=torch.sigmoid(model(tstx)).round()\n",
        "        print('testing report:')\n",
        "        print(classification_report(preds.view(preds.shape[0]), tsty))\n",
        "        print(torch.sigmoid(preds).round().shape)"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "K-GCeSQGH6dA"
      },
      "source": [
        "#tstx=torch.from_numpy(x_train_transformed_9_features[600:]).float()\n",
        "#tstx=torch.from_numpy(x_train[600:]).float()\n",
        "tsty=torch.from_numpy(y_train[600:]).float()\n",
        "\n",
        "#test(model,tstx,tsty)"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "PnfCcJo9H6dB"
      },
      "source": [
        "def run_network(model, trainx,trainy, testx, testy):\n",
        "    model=Model()\n",
        "    criterion = nn.BCEWithLogitsLoss()\n",
        "    optimizer=torch.optim.SGD(model.parameters(),lr=0.6,momentum=0.99, nesterov=True)\n",
        "    #optim=torch.optim.Adam(model.parameters(),lr=0.1)\n",
        "    scheduler=torch.optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.2)\n",
        "    model, losses= train(model,optimizer, scheduler, criterion,trainx, trainy)\n",
        "    test(model, testx, testy)\n",
        "    \n",
        "    return model, losses"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Hh_j13-H6dB",
        "outputId": "0abc9333-384c-44f5-8907-73db4f9df6ef"
      },
      "source": [
        "fold=0\n",
        "losses=[[] for i in range(10)]\n",
        "\n",
        "for trids, tstids in k_fold.split(x_train_transformed_9_features):\n",
        "    trainx, testx=x_train_transformed_9_features[trids,:], x_train_transformed_9_features[tstids,:]\n",
        "    #trainx, testx=x_train[trids,:], x_train[tstids,:]\n",
        "    trainy, testy=y_train[trids], y_train[tstids]\n",
        "    \n",
        "    trainx=torch.from_numpy(trainx).float()\n",
        "    trainy=torch.from_numpy(trainy).float()\n",
        "    testx=torch.from_numpy(testx).float()\n",
        "    testy=torch.from_numpy(testy).float()\n",
        "    \n",
        "    print('fold: ',fold)\n",
        "    \n",
        "    model=Model()\n",
        "    \n",
        "    _,losses[fold]=run_network(model, trainx,trainy, testx,testy)\n",
        "    \n",
        "    fold=fold+1\n",
        "\n",
        "\n"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fold:  0\n",
            "\n",
            "training report: \n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.51      0.85      0.63       106\n",
            "         1.0       0.97      0.85      0.90       574\n",
            "\n",
            "    accuracy                           0.85       680\n",
            "   macro avg       0.74      0.85      0.77       680\n",
            "weighted avg       0.90      0.85      0.86       680\n",
            "\n",
            "testing report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.36      1.00      0.53         5\n",
            "         1.0       1.00      0.87      0.93        71\n",
            "\n",
            "    accuracy                           0.88        76\n",
            "   macro avg       0.68      0.94      0.73        76\n",
            "weighted avg       0.96      0.88      0.91        76\n",
            "\n",
            "torch.Size([76, 1])\n",
            "fold:  1\n",
            "\n",
            "training report: \n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.50      0.84      0.63       102\n",
            "         1.0       0.97      0.85      0.91       578\n",
            "\n",
            "    accuracy                           0.85       680\n",
            "   macro avg       0.73      0.85      0.77       680\n",
            "weighted avg       0.90      0.85      0.86       680\n",
            "\n",
            "testing report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.35      0.78      0.48         9\n",
            "         1.0       0.96      0.81      0.88        67\n",
            "\n",
            "    accuracy                           0.80        76\n",
            "   macro avg       0.66      0.79      0.68        76\n",
            "weighted avg       0.89      0.80      0.83        76\n",
            "\n",
            "torch.Size([76, 1])\n",
            "fold:  2\n",
            "\n",
            "training report: \n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.47      0.84      0.60        97\n",
            "         1.0       0.97      0.84      0.90       583\n",
            "\n",
            "    accuracy                           0.84       680\n",
            "   macro avg       0.72      0.84      0.75       680\n",
            "weighted avg       0.90      0.84      0.86       680\n",
            "\n",
            "testing report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.61      0.85      0.71        13\n",
            "         1.0       0.97      0.89      0.93        63\n",
            "\n",
            "    accuracy                           0.88        76\n",
            "   macro avg       0.79      0.87      0.82        76\n",
            "weighted avg       0.90      0.88      0.89        76\n",
            "\n",
            "torch.Size([76, 1])\n",
            "fold:  3\n",
            "\n",
            "training report: \n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.48      0.79      0.59       105\n",
            "         1.0       0.96      0.84      0.90       575\n",
            "\n",
            "    accuracy                           0.83       680\n",
            "   macro avg       0.72      0.82      0.75       680\n",
            "weighted avg       0.88      0.83      0.85       680\n",
            "\n",
            "testing report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.50      0.82      0.62        11\n",
            "         1.0       0.97      0.86      0.91        65\n",
            "\n",
            "    accuracy                           0.86        76\n",
            "   macro avg       0.73      0.84      0.77        76\n",
            "weighted avg       0.90      0.86      0.87        76\n",
            "\n",
            "torch.Size([76, 1])\n",
            "fold:  4\n",
            "\n",
            "training report: \n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.47      0.82      0.60       101\n",
            "         1.0       0.96      0.84      0.90       579\n",
            "\n",
            "    accuracy                           0.84       680\n",
            "   macro avg       0.72      0.83      0.75       680\n",
            "weighted avg       0.89      0.84      0.85       680\n",
            "\n",
            "testing report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.40      0.86      0.55         7\n",
            "         1.0       0.98      0.87      0.92        69\n",
            "\n",
            "    accuracy                           0.87        76\n",
            "   macro avg       0.69      0.86      0.73        76\n",
            "weighted avg       0.93      0.87      0.89        76\n",
            "\n",
            "torch.Size([76, 1])\n",
            "fold:  5\n",
            "\n",
            "training report: \n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.50      0.84      0.63       102\n",
            "         1.0       0.97      0.85      0.91       578\n",
            "\n",
            "    accuracy                           0.85       680\n",
            "   macro avg       0.74      0.85      0.77       680\n",
            "weighted avg       0.90      0.85      0.87       680\n",
            "\n",
            "testing report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.48      0.83      0.61        12\n",
            "         1.0       0.96      0.83      0.89        64\n",
            "\n",
            "    accuracy                           0.83        76\n",
            "   macro avg       0.72      0.83      0.75        76\n",
            "weighted avg       0.89      0.83      0.85        76\n",
            "\n",
            "torch.Size([76, 1])\n",
            "fold:  6\n",
            "\n",
            "training report: \n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.47      0.81      0.60        96\n",
            "         1.0       0.97      0.85      0.90       585\n",
            "\n",
            "    accuracy                           0.84       681\n",
            "   macro avg       0.72      0.83      0.75       681\n",
            "weighted avg       0.90      0.84      0.86       681\n",
            "\n",
            "testing report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.42      0.92      0.58        12\n",
            "         1.0       0.98      0.76      0.86        63\n",
            "\n",
            "    accuracy                           0.79        75\n",
            "   macro avg       0.70      0.84      0.72        75\n",
            "weighted avg       0.89      0.79      0.81        75\n",
            "\n",
            "torch.Size([75, 1])\n",
            "fold:  7\n",
            "\n",
            "training report: \n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.49      0.87      0.63       100\n",
            "         1.0       0.97      0.84      0.90       581\n",
            "\n",
            "    accuracy                           0.85       681\n",
            "   macro avg       0.73      0.86      0.76       681\n",
            "weighted avg       0.90      0.85      0.86       681\n",
            "\n",
            "testing report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.43      0.67      0.52         9\n",
            "         1.0       0.95      0.88      0.91        66\n",
            "\n",
            "    accuracy                           0.85        75\n",
            "   macro avg       0.69      0.77      0.72        75\n",
            "weighted avg       0.89      0.85      0.87        75\n",
            "\n",
            "torch.Size([75, 1])\n",
            "fold:  8\n",
            "\n",
            "training report: \n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.47      0.85      0.60        95\n",
            "         1.0       0.97      0.84      0.90       586\n",
            "\n",
            "    accuracy                           0.84       681\n",
            "   macro avg       0.72      0.85      0.75       681\n",
            "weighted avg       0.90      0.84      0.86       681\n",
            "\n",
            "testing report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.53      0.77      0.62        13\n",
            "         1.0       0.95      0.85      0.90        62\n",
            "\n",
            "    accuracy                           0.84        75\n",
            "   macro avg       0.74      0.81      0.76        75\n",
            "weighted avg       0.87      0.84      0.85        75\n",
            "\n",
            "torch.Size([75, 1])\n",
            "fold:  9\n",
            "\n",
            "training report: \n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.46      0.82      0.59        93\n",
            "         1.0       0.97      0.85      0.90       588\n",
            "\n",
            "    accuracy                           0.84       681\n",
            "   macro avg       0.71      0.83      0.75       681\n",
            "weighted avg       0.90      0.84      0.86       681\n",
            "\n",
            "testing report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.56      0.94      0.70        16\n",
            "         1.0       0.98      0.80      0.88        59\n",
            "\n",
            "    accuracy                           0.83        75\n",
            "   macro avg       0.77      0.87      0.79        75\n",
            "weighted avg       0.89      0.83      0.84        75\n",
            "\n",
            "torch.Size([75, 1])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "hEj4rX9xH6dC"
      },
      "source": [
        ""
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 298
        },
        "id": "yImmOUVaH6dC",
        "outputId": "0ca36d50-b5fe-4236-f30d-88349c5116c2"
      },
      "source": [
        "for i in range(10):\n",
        "  plt.plot(losses[i])\n",
        "plt.title('losses')\n",
        "plt.ylim(0.2,1)"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.2, 1.0)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "Muup2X94H6dC"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}